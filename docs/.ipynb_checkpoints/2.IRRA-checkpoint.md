#  Paper Information

- __Paper title__: *Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval*
- __Conference__: CVPR 2023
- __Link github__: https://github.com/anosorae/IRRA
- __Link paper__: https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Cross-Modal_Implicit_Relation_Reasoning_and_Aligning_for_Text-to-Image_Person_Retrieval_CVPR_2023_paper.pdf
- __Benchmark ranking__: Top $3^{rd}$ on CUHK-PEDES and RSTPReid benchmark

# Overview/Main contribution

![image.png](docs/2.IRRA_files/e5f9eaf2-2fbb-4f81-8898-4a578dddade3.png)

1. Global matching:
    - map images and texts global features into a joint embedding space
    - using matching losses only at the end of the network
    - __Drawbacks__: do not take advantage of the local features in the middle-level layers, low performance.
2. Local matching:
    - align on local features (body part in image and textual entities
    - better retrieval performance
    - __Drawbacks__: unavoidable noise and uncertainty problems because Image local feature vectors do not always have a 1-1 relationship with image feature vectors. resource-demanding because requires extracting and storing multiple local part representations of images and texts, computing pairwise similarity during inference
3. IRRA:
    - builds relations between visual and textual representations through self- and cross-attention mechanisms and masked language modeling (MLM) task
    - computes only one global image-text pair similarity score in the inference stage


## Main contribution

- Introduce a new loss named image-text __similarity distribution matching (SDM)__, used for image-text constrastive learning

- Utilize __CLIP__ - a *Vision-Language Pretraining (VLP)* model, as backbone
    - CLIP is a powerful backbone that was trained on a 400M pairs of image-text by contrastive learning method
    - CLIP is powerful enough to be used as a backbone -> don't need to create new large-scale dataset for general pretraining
    - Just focus on downstream task

        

- Use 3 objective function for finetuning on TBPR dataset:
    - SDM loss
    - MLM loss (similar to paper APTM)
    - ID loss (due to the available of ID label in all TBPR dataset)


- High results on three benchmarks: CUHK-PEDES, ICFG-PEDES and RSTPReid.

# Model Architecture / Method

__Re-draw model (done by Danh)__:

Link to whiteboard: https://miro.com/app/board/uXjVNVZpe9Y=/?share_link_id=620212020225

## Architecture

![image.png](docs/2.IRRA_files/e7567504-965e-4a78-bfad-562f80f3ac38.png)

![image.png](docs/2.IRRA_files/e7f4964b-85de-43bf-866c-22f03ba0be1d.png)

- Typical architecture: 3 encoder
    - A __vision encoder__: CLIP vision encoder
        - CLIP vision encoder has two version:
            - Version 1: Vision Transformer
            - Version 2: ResNet 
        - IRRA use Vision Transformer with CLIP pretrained parameter as default
        - Encode an image into a sequence of patch embedding, with extra __CLS__ token embedding at the start of the sequence
            - Behave like vision encoder of APTM
            - Use __CLS__ token as the global representation for the whole image
            - Need a linear layer to project the output __CLS__ embedding into a common space
    - A __language encoder__: a standard transformer-encoder with 12 encoder layers
        - Tokenize the text input into sequence of tokens by the lower-cased byte pair encoding (BPE) algorithm
        - Use extra __SOS__ token at the start of the sequence, __EOS__ token at the end of the sequence (NOT use CLS token)
        - __EOS__ token is used as global representation for the whole text (the role is similar to the CLS in BERT)
        - Need a linear layer to project the output __EOS__ token embedding to the common space
    - A cross encoder: 
        - Consist of a cross-attention module which use text representation as query, visual represention as key & value 
            - Place the Layer Normalize layer before the cross-attention layer (pre-norm)
        - Following cross-attention module is the standard transformer-encoder with 4 encoder layers
        - The output __EOS__ token embedding is used as global representation for image-text pair (behave like __CLS__ in APTM)
        

## Objective Functions

### Implicit Relation Reasoning

- Just Masked Lanugage Modeling (MLM) objective
    - The output embedding vectors from cross encoder for masked positions is used feed to a MLP for prediction
    - Consider as the problem of classification of the masked tokens.
    - Using Cross Entropy Loss


### Similarity Distribution Matching

![image.png](docs/2.IRRA_files/e018b1eb-50a0-459f-ad46-01ddd1d004d1.png)

- Let:
    - Minibatch is N pair of positive text-image, can be represented by NxN matrix
    - $f_i^v$ be the vector embedding of the $i^{th}$ image in minibatch
    - $f_j^t$ be the vector embedding of the $j^{th}$ text in minibatch
    - These vector embedding are output of encoders

- Similarity score between image and text usually calculate __in bi-directional__ manner:
    - image-to-text
    - text-to-image

- For image-to-text (loss is calculate per image)
    - Consider as image classfication problem, classify image belong to which text (class) in a list of text
    - Calculate similarity score by __cosine__ function ($sim$)
    - Smooth the score by using a __temperature hyperparameter__ $\tau$
        $$\operatorname{sim}\left(f_i^v, f_j^t\right) / \tau$$
    - Scale by __softmax__ for dimension of text
        \
        $\rightarrow p_{i,j} \in [0,1]$
    $$p_{i, j}=\frac{\exp \left(\operatorname{sim}\left(f_i^v, f_j^t\right) / \tau\right)}{\sum_{k=1}^N \exp \left(\operatorname{sim}\left(f_i^v, f_k^t\right) / \tau\right)}$$ 

- Can consider this is the probabilities of pair of image $i$ belong to text $j$ 
    - $[(p_{i,j})_{j=1}^{N}]$ is a __probability distribution vector__ of image $i$
    - Let $[(q_{i,j})_{j=1}^{N}]$ is the __ground-true probability distribution__ vector
    - In the other papers, just apply cross entropy loss and call the loss as *constrastive loss*!
    - In this paper, entropy loss is replaced by __KL Divergence__ loss, called SDM loss

- Similarity distribution matching (SDM) loss for a minibatch, image-to-text direction:
    - For each image i, calcute for the loss per each image
    - Then accumulate for all images in batch
    $$\mathcal{L}_{i 2 t}=K L\left(\mathbf{p}_{\mathbf{i}}|| \mathbf{q}_{\mathbf{i}}\right)=\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N p_{i, j} \log \left(\frac{p_{i, j}}{q_{i, j}+\epsilon}\right)$$

- For text to image, $\mathcal{L}_{t2i}$ is calculate in similar way
- Final SDM loss: $$\mathcal{L}_{SDM} = \frac {\mathcal{L}_{i2t} + \mathcal{L}_{t2i}}{2}$$

![image.png](docs/2.IRRA_files/13d3a4d7-7cb9-4a74-92ca-31a16b5c28d4.png)

### ID Loss

- Because, each image and caption has ID labels in datasets.
- Can consider as classification image and text to the corresponding classes.
- Using Cross Entropy Loss

## Final loss   


$$\mathcal{L} = \mathcal{L}_{SDM} + \mathcal{L}_{MLM} + \mathcal{L}_{ID}$$

# Training phase

## Dataset & Data Augmentation

- For image:
    - Random horizontally flipping
    - Random crop with padding
    - Random erasing

## Implemention detail

- For each layer of the cross encoder, the hidden size and number of heads are set to 512 and 8.
- All input images are resized to 384 × 128. 
- The maximum length of the textual token sequence L is set to 77
- Adam optimizer for 60 epochs with a
learning rate initialized to $10^{−5}$ and cosine learning rate
decay.
- Perform experiments on a single RTX3090 24GB GPU!

## Evaluation result

### Evaluate the efficiency of each loss

Why choose SDM ? Compare SDM to SMPM loss (a common constrastive loss)

![image.png](docs/2.IRRA_files/654d8e6a-2619-4099-87ec-ddb739902c0a.png)

# Inference phase

# Conclusion

## New points in this paper

## Pro

## Cons

## How to improve?

# Demo in notebook

## Set up

### Define path


```python
import os, sys
print(os.getcwd())
from pathlib import Path
ROOT_PATH = Path('../paper_clones/IRRA').resolve()
sys.path.append(str(ROOT_PATH))
IMAGE_PATH = Path('../DATASET').resolve()
sys.path.append(str(IMAGE_PATH))
```

    /home/jovyan/workspace/ba_pre_thesis/report



```python
MODEL_PATH = ROOT_PATH/'MODEL'/'rstp'/'best.pth'
```

### Import libries / local modules


```python
from prettytable import PrettyTable
from torchinfo import summary
import os
import torch
import numpy as np
import time
import os.path as op
from datasets import build_dataloader
from processor.processor import do_inference
from utils.checkpoint import Checkpointer
from utils.logger import setup_logger
from model import build_model
from utils.metrics import Evaluator
import argparse
from utils.iotools import load_train_configs
from utils.checkpoint import Checkpointer
from datasets import build_dataloader
```

    /home/jovyan/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


### Load config for finetunning on RSTPReid


```python
config_file = ROOT_PATH/'MODEL/rstp/configs.yaml'
args = load_train_configs(config_file)
args.root_dir = IMAGE_PATH
args.training = True
args.batch_size = 8
```

### Build model & load checkpoint


```python
args.training = True
train_model = model = build_model(args, num_classes=3701)

# args.training = False
# test_model = build_model(args, num_classes=3701)
# test_model = test_model.eval()
```

    Training Model with ['sdm', 'id', 'mlm'] tasks
    Resized position embedding from size:torch.Size([1, 197, 768]) to size: torch.Size([1, 193, 768]) with height:24 width: 8


    /opt/conda/envs/aptm/lib/python3.8/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
      warnings.warn(



```python
checkpointer = Checkpointer(model)
checkpointer.load(MODEL_PATH)
# checkpointer = Checkpointer(test_model)
# checkpointer.load(MODEL_PATH)
```

## View batch data


```python
args.training = True
train_loader, val_img_loader, val_txt_loader, num_classes = build_dataloader(args)
train_batch = next(iter(train_loader))
print(len(train_batch))
print(train_batch.keys())
list_keys = list(train_batch.keys())
temp = [print(key, train_batch[key].shape) for key in list_keys]
```

    6
    dict_keys(['mlm_labels', 'caption_ids', 'images', 'image_ids', 'mlm_ids', 'pids'])
    mlm_labels torch.Size([8, 77])
    caption_ids torch.Size([8, 77])
    images torch.Size([8, 3, 384, 128])
    image_ids torch.Size([8])
    mlm_ids torch.Size([8, 77])
    pids torch.Size([8])



```python
# args.training = False
# test_img_loader, test_txt_loader, num_classes = build_dataloader(args)
```

## Summary model

__Get name of model's module__


```python
modules = [x for x in dict(model.named_modules()).keys() if '.' not in x and len(x) > 0]
modules
```




    ['base_model',
     'classifier',
     'cross_attn',
     'cross_modal_transformer',
     'ln_pre_t',
     'ln_pre_i',
     'ln_post',
     'mlm_head']




```python
summary(model)
```




    ====================================================================================================
    Layer (type:depth-idx)                                                      Param #
    ====================================================================================================
    IRRA                                                                        --
    ├─CLIP: 1-1                                                                 301,568
    │    └─VisionTransformer: 2-1                                               542,208
    │    │    └─Conv2d: 3-1                                                     589,824
    │    │    └─LayerNorm: 3-2                                                  1,536
    │    │    └─Transformer: 3-3                                                85,054,464
    │    │    └─LayerNorm: 3-4                                                  1,536
    │    └─Transformer: 2-2                                                     --
    │    │    └─Sequential: 3-5                                                 37,828,608
    │    └─Embedding: 2-3                                                       25,296,896
    │    └─LayerNorm: 2-4                                                       1,024
    ├─Linear: 1-2                                                               1,898,613
    ├─MultiheadAttention: 1-3                                                   787,968
    │    └─NonDynamicallyQuantizableLinear: 2-5                                 262,656
    ├─Transformer: 1-4                                                          --
    │    └─Sequential: 2-6                                                      --
    │    │    └─ResidualAttentionBlock: 3-6                                     3,152,384
    │    │    └─ResidualAttentionBlock: 3-7                                     3,152,384
    │    │    └─ResidualAttentionBlock: 3-8                                     3,152,384
    │    │    └─ResidualAttentionBlock: 3-9                                     3,152,384
    ├─LayerNorm: 1-5                                                            1,024
    ├─LayerNorm: 1-6                                                            1,024
    ├─LayerNorm: 1-7                                                            1,024
    ├─Sequential: 1-8                                                           --
    │    └─Linear: 2-7                                                          262,656
    │    └─QuickGELU: 2-8                                                       --
    │    └─LayerNorm: 2-9                                                       1,024
    │    └─Linear: 2-10                                                         25,346,304
    ====================================================================================================
    Total params: 190,789,493
    Trainable params: 190,789,493
    Non-trainable params: 0
    ====================================================================================================



__Base model__


```python
summary(model.base_model)
```




    ===============================================================================================
    Layer (type:depth-idx)                                                 Param #
    ===============================================================================================
    CLIP                                                                   301,568
    ├─VisionTransformer: 1-1                                               542,208
    │    └─Conv2d: 2-1                                                     589,824
    │    └─LayerNorm: 2-2                                                  1,536
    │    └─Transformer: 2-3                                                --
    │    │    └─Sequential: 3-1                                            85,054,464
    │    └─LayerNorm: 2-4                                                  1,536
    ├─Transformer: 1-2                                                     --
    │    └─Sequential: 2-5                                                 --
    │    │    └─ResidualAttentionBlock: 3-2                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-3                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-4                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-5                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-6                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-7                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-8                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-9                                3,152,384
    │    │    └─ResidualAttentionBlock: 3-10                               3,152,384
    │    │    └─ResidualAttentionBlock: 3-11                               3,152,384
    │    │    └─ResidualAttentionBlock: 3-12                               3,152,384
    │    │    └─ResidualAttentionBlock: 3-13                               3,152,384
    ├─Embedding: 1-3                                                       25,296,896
    ├─LayerNorm: 1-4                                                       1,024
    ===============================================================================================
    Total params: 149,617,664
    Trainable params: 149,617,664
    Non-trainable params: 0
    ===============================================================================================



__ID classfier__


```python
print(model.classifier)
```

    Linear(in_features=512, out_features=3701, bias=True)


__MLP head__


```python
model.mlm_head
```




    Sequential(
      (dense): Linear(in_features=512, out_features=512, bias=True)
      (gelu): QuickGELU()
      (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (fc): Linear(in_features=512, out_features=49408, bias=True)
    )



__Cross Encoder__ (cross-attention module + 4 transformer encoder layers)


```python
summary(model.cross_attn)
```




    ======================================================================
    Layer (type:depth-idx)                        Param #
    ======================================================================
    MultiheadAttention                            787,968
    ├─NonDynamicallyQuantizableLinear: 1-1        262,656
    ======================================================================
    Total params: 1,050,624
    Trainable params: 1,050,624
    Non-trainable params: 0
    ======================================================================




```python
summary(model.cross_modal_transformer)
```




    =====================================================================================
    Layer (type:depth-idx)                                       Param #
    =====================================================================================
    Transformer                                                  --
    ├─Sequential: 1-1                                            --
    │    └─ResidualAttentionBlock: 2-1                           --
    │    │    └─MultiheadAttention: 3-1                          1,050,624
    │    │    └─LayerNorm: 3-2                                   1,024
    │    │    └─Sequential: 3-3                                  2,099,712
    │    │    └─LayerNorm: 3-4                                   1,024
    │    └─ResidualAttentionBlock: 2-2                           --
    │    │    └─MultiheadAttention: 3-5                          1,050,624
    │    │    └─LayerNorm: 3-6                                   1,024
    │    │    └─Sequential: 3-7                                  2,099,712
    │    │    └─LayerNorm: 3-8                                   1,024
    │    └─ResidualAttentionBlock: 2-3                           --
    │    │    └─MultiheadAttention: 3-9                          1,050,624
    │    │    └─LayerNorm: 3-10                                  1,024
    │    │    └─Sequential: 3-11                                 2,099,712
    │    │    └─LayerNorm: 3-12                                  1,024
    │    └─ResidualAttentionBlock: 2-4                           --
    │    │    └─MultiheadAttention: 3-13                         1,050,624
    │    │    └─LayerNorm: 3-14                                  1,024
    │    │    └─Sequential: 3-15                                 2,099,712
    │    │    └─LayerNorm: 3-16                                  1,024
    =====================================================================================
    Total params: 12,609,536
    Trainable params: 12,609,536
    Non-trainable params: 0
    =====================================================================================



## Inference thourgh train model


```python
cuda_train_batch = {key: value.cuda() for key, value in train_batch.items()}
model = model.cuda()
res = model(cuda_train_batch)
```


```python
res
```




    {'temperature': tensor(0.0200),
     'sdm_loss': tensor(4.4844, device='cuda:0', grad_fn=<AddBackward0>),
     'id_loss': tensor(7.5057, device='cuda:0', grad_fn=<MulBackward0>),
     'img_acc': tensor(0.8750, device='cuda:0'),
     'txt_acc': tensor(0.6250, device='cuda:0'),
     'mlm_loss': tensor(1.2606, device='cuda:0', grad_fn=<MulBackward0>),
     'mlm_acc': tensor(0.6857, device='cuda:0')}



## Print architecture image


```python
from torchview import draw_graph
model_graph = draw_graph(model, input_data=[cuda_train_batch],  expand_nested=True, graph_name='IRRA', save_graph=True, filename='IRRA_architecture', directory='architecture_image')
```

## Loss

## Training loop

## Inference


```python

```
