#  Paper Information

- __Paper title__: PLIP: Language-Image Pre-training for Person Representation Learning
- __Date Published__: 15/5/2023
- __Conference__: not yet!
- __Link github__: https://github.com/Zplusdragon/PLIP
- __Link paper__: http://export.arxiv.org/abs/2305.08386
- Why should read this paper?
    - Create the largest and high quality dataset for pretraining: 4.5M
    - Propose a new strategy to generate caption for dataset based on GPT model
    - Powerful backbone can be adapted to various tasks: PAR, Re-ID, TBPR

# Overview/Main contribution

## Main contribution

- Construct a large-scale person dataset named SYNTH-PEDES, which consist 4.5M pairs of (image, text)

- Create a new backbone __PLIP__ on new dataset for general pretraining
    - __PLIP__ is just a backbone, need a model baseline to used transfered represenation for TBPR finetunning
    - Baselines choosen for finetunning: 
        - CMPC/C
        - LGUR

- Design 3 pretext task for training PLIP:
    - Semantic-fused Image Colorization: 
        - Given a textual description and greyscale version of the matched image
        - Try to restore the color information of a grayscale person image
    - Visual-fused Attributes Prediction: 
        - Given a masked attributes version of textual description, and it's corresponed colorful image
        - Try to predict the masked attributes phrases in textual descriptions
    - Vision-language Constrastive Learning

- Backbone is powerful that can be adapted for 3 downstream task:
    - Text-based Person Retrieval
    - Image-based Peron Re-Identification
    - Pedestrian Attribute Recogition

## Recall ancestor/baseline papers

### CMPC/C

### LGUR

# SYNTH-PEDES dataset

## Overview dataset

- SYNTH-PEDES is the largest dataset for TBPR:
    - Based on LUPerson-NL and LPW, two large-scale dataset for Image-based Person Re-Identification
        - Create and train a powerful model named SPAC for image captioning on LUPerson-NL
        - Stylish Pedestrian Attributesunion Captioning (SPAC) mainly comprises two modules:
            - Image Encoder: ResNet101-FPN
            - Caption Generator (decoder): GPT2
    - Size:
        - About 4.5M images, 3x the size of MALS (in APTM)
        - 12M text decription
    - Compared to MALS dataset in APTM
        - SYNTH-PEDES is high quality and more realistic than MALS
    - Each image also has the ID of person
    - Camera view is diversity

![image.png](docs/4.PLIP_files/180e846b-03d1-41de-9ece-33b0ec72ad04.png)

## Label format

Each image have:
- ID of person
- List of attribute keyword
    - Keyword is dynamic (constrast to fixed label of PAR dataset) 
    - Length of keywords list: 6
- Caption text: 1 or 2 captions for each image
- Attribute prompt caption: 
    - Just fill the attribute keywords into blanks of the predefined template
    - --> Each image has 2 or 3 style of captions
    ![image.png](docs/4.PLIP_files/07144b91-2fb5-4b5b-9d93-8fe66957ab0c.png)

__Labels from json file__

```json
[
    {
        "id": 1,
        "file_path": "Part1/1/0.jpg",
        "attributes": [
            "woman,short hair,black jacket,blue denim jeans,black sneakers,black backpack"
        ],
        "captions": [
            "A woman with black hair and she is wearing a black jacket with blue jeans paired with black shoes."
        ],
        "prompt_caption": [
            "The woman has short hair. She is wearing a black jacket, blue denim jeans and black sneakers. She is carrying a black backpack. "
        ]
    },
    
    {
        "id": 1,
        "file_path": "Part1/1/1.jpg",
        "attributes": [
            "girl,dark hair,black jacket,blue jeans,black shoes,unknown"
        ],
        "captions": [
            "A woman wearing a black jacket over a Gray shirt, a pair of blue jeans and a pair of black shoes."
        ],
        "prompt_caption": [
            "A girl with dark hair wearing a black jacket and blue jeans with black shoes."
        ]
    }, ...]
```

## Construction process

__Step 1__: Manual labeling
- Collected person Re-ID datasets LUPerson-NL and LPW
- Filter the noise from LUPerson-NL, which contain some error and noise
- Before training SPACE, label for small subset of dataset to make datase

__Step 2__: Train model SPAC to generate a texual caption and list of 6 attribute keywords for image


__Step 3__: Attributes Prompt Engineering
- After step 2, each image has a caption, and a list of generated attribute keywords
- Adding a new style of caption for each image, based on the attribute keywords that was generated by SPAC
    - Just by fill-in-blank method
![image.png](docs/4.PLIP_files/466b5f1e-53ac-4af7-8b1a-566eabfc7473.png)

## Stylish Pedestrian Attributes union Captioning (SPAC)

- Stylish Pedestrian Attributesunion Captioning (SPAC) mainly comprises two modules: a prefix encoder and a shared generator. 
    - Specifically, use ResNet101-FPN as the encoder and GPT2 as the generator

- What is __prefix encoder__?
    - Use output of embedding vectors of an encoder to use as prefix queries
    - Input of the decoder is the prefix queries + learnable input queries of decoder
        - Bi-directional attention is applied for prefix queries
            - Behave as cross-attention between encoder  decoder
        - Left-to-right/Casual attention is applied for the remain input queries of decoder


![image.png](docs/4.PLIP_files/e294a52a-bca5-4ac0-a1a9-dc45795044d8.png)

### Architecture of SPAC

- Prefix image encoder $PE$: ResNet101-FPN
- $m$ styled encoder $SE$: each encoder encode the image to different styled embedding, will be fed into a shared decoder to generated m different styled of text caption
- A shared decoder $G$ for different styled of text caption and 6 type of attribute phrases

### Training SPAC

- Ground-true data: triplet of (image, list of attribute keywords, caption $\{x^i, A^i, y^i\}_{i=1}^{N}$
    - $A^i$ is list of 6 attribute keywords,
        - e.g. {man, black hair, white shirt, yellow trousers, brown shoes}
- Consider 6 attribute phares and 1 texual caption ---> as 7 sequence (by adding some padding token)
    - Ground-true label for each image $x_i$ is the list of 7 sequence $\{s_1, s_2, \dotsc, s_7\}$


__Data flow for generating different captions for each image__

- Feed image $x$ into prefix encoder and n different branches:
    - $7$ branches:
        - 6 for 6 attribute phrases 
        - $1$ for text caption
    - How branches are implemented? 
        - Not shown in paper and source code!
        - Perhap a MLP!
        - $e_k$ is the ouput embedding vector of branch $k$, correspond to attribute phrase $k$
        - $n=7$
    $$ \{e_1, e_2, \dotsc, e_7\} = \{ BR_{k}(PE(x)) \}_{k=1}^7$$
    - Consider $e_{concat}$ as concat vector of 7 vectors

- Feed concat vector into $m$ styled encoders:
    - $c_k$ is the ouput embedding vector of styled encoder $k$
    - How styled encoder is implemented ?
        - Each styled encoder receive only an concat input vector
        - Perhap MLP!
    - In paper $m$ = 2 --> generate two styled of text caption for each image
    $$\mathbf{C}=\{c_1, c_2, \dotsc, c_m\} = \{SE(e_{concat}) \}_{i=1}^m$$

![image.png](docs/4.PLIP_files/14099d8e-f2c9-4621-8885-2c40ed3976cc.png)

- Let $Q_{style}= \{ q_1, q_2, \dotsc, q_{l_{max}} \}$ the casual input queries of decoder $G$:
    - For each style:
        - $c_{style}$: prefix query
        - $Q$: casual query (length = $l_{max}$)
$$Y = sequence \{ y_1, y_2, \dotsc, y_{l_{max}}    \} = G(concat([c_{style}, Q_{style}]))$$
$$\mathcal{L}_{style}=\sum_{j=1}^{\ell_{max}}- \log G\left(y_j \mid c_{style}, y_1, \ldots, y_{j-1}\right)$$


- Final loss for generate diffentes style of captions
$$\mathcal{L}_{caption} = \sum\mathcal{L}_{style}$$

__Flow for generating attributes phrases (keywords)__

![image.png](docs/4.PLIP_files/4d6824ae-9756-46dc-b747-1d805802b60d.png)

- Let $Q_{att}= \{ q_1, q_2, \dotsc, q_{l_{max}} \}$ the casual input queries of decoder $G$:
    - For attribute phrase $k$
        - $e_{k}$: attribute embedding from branch $k$
        - $Q$: casual query (length = $l_{max}$)
$$Y = sequence \{ y_1, y_2, \dotsc, y_{l_{max}}    \} = G(concat([e_{k}, Q]))$$
$$\mathcal{L}_{attr-k}=\sum_{j=1}^{\ell_{max}}- \log G\left(y_j \mid c_{style}, y_1, \ldots, y_{j-1}\right)$$

- Final loss for generate diffentes style of captions
$$\mathcal{L}_{attribute} = \sum\mathcal{L}_{attr-k}$$

__Final loss for SPAC__
$$\mathcal{L_{generate}} = \lambda \mathcal{L}_{attribute} + \mathcal{L}_{caption} $$

![image.png](docs/4.PLIP_files/1592ed78-ce9d-4092-b84b-e9d13f1c2452.png)


# PLIP Architecture

## Architecture

![image.png](docs/4.PLIP_files/729f1b1d-87d1-4cf4-afee-d830d0df1772.png)

__Vision Encoder__

- Vision Encoder: ResNet Feature Pyramid Network 
    - Output of encoder is 4 3-d tensor of feature maps that come from 4 different blocks in ResNet
![image.png](docs/4.PLIP_files/aaa0d158-0010-4979-866d-b77d37103f1e.png)

__Text Encoder__
- Text Encoder: BERT base

__Additional module for learning pretext task__:
- Vision-Matching Matching (VLM): just a Constrastive Learning module

    
- Visual-fused Attributes Prediction (VAP): module learn masked text prediction from image clues
    
- Semantic-fused Image Colorization (SIC): module learn image colorization from text clues

## Objective Functions

### Vision-Language Matching (VLM)

Just other name of vision-language constrastive learning

__Global visual feature vector__

- The feature maps with smalles resolution that comes out the last block of ResNet: $F_{last}$: CxHxW
- $F_{last}$:
    - --> tokenization --> (HW)xC
    - -->add global pooling feature --> (HW+1)xC
    - --> multi-head self-attention --> $v_{g}$: 1xC 

__Global text feature vector__

- Just __CLS__ 's output embedding from BERT -->global text feature vector $t_{g}$

__Recall the CMPM loss__

- A comon loss used for constrastive learning

- From paper "Deep Cross-Modal Projection Learning for Image-Text Matching", ECCV 2018

- Given a mini-batch with $n$ image and text samples, for each image $\boldsymbol{x}_i$ the image-text pairs are constructed as $\left\{\left(\boldsymbol{x}_i, \boldsymbol{z}_j\right), y_{i, j}\right\}_{j=1}^n$, where $y_{i, j}=1$ means that $\left(\boldsymbol{x}_i, \boldsymbol{z}_j\right)$ is a matched pair, while $y_{i, j}=0$ indicates the unmatched ones. The probability of matching $\boldsymbol{x}_i$ to $\boldsymbol{z}_j$ is defined as:
$$
p_{i, j}=\frac{\exp \left(\boldsymbol{x}_i^{\top} \overline{\boldsymbol{z}}_j\right)}{\sum_{k=1}^n \exp \left(\boldsymbol{x}_i^{\top} \overline{\boldsymbol{z}}_k\right)} \quad \text { s.t. } \overline{\boldsymbol{z}}_j=\frac{\boldsymbol{z}_j}{\left\|\boldsymbol{z}_j\right\|}
$$
- Normalized Ground-true vector: $$q_{i, j}=\frac{y_{i, j}}{\sum_{k=1}^n y_{i, k}}$$
- The matching loss of associating $\boldsymbol{x}_i$ with correctly matched text samples is defined as
$$
\mathcal{L}_i=  KL(\mathbf{p_i}|| \mathbf{q_i})= \sum_{j=1}^n p_{i, j} \log \frac{p_{i, j}}{q_{i, j}+\epsilon}
$$
where $\epsilon$ is a small number to avoid numerical problems, and the matching loss from image to text in a mini-batch is computed by
$$
\mathcal{L}_{i 2 t}=\frac{1}{n} \sum_{i=1}^n \mathcal{L}_i
$$


$$\mathcal{L}_{CMPM} = \mathcal{L}_{i2t} + \mathcal{L}_{t2i}$$

- What is different about CMPM vs SDM in IRRA?
    - CMPM similarity function: 
        - $sim(v_1, v_2) = \text{length projected vector of } v_1 \text{ on } v_2$
    - IRRA similarity function: 
        - $sim(v_1, v_2) = cosine(v_1, v_2)$
    - IRRA paper show that SDM is better than CMPM

__PLIP apply CMPM loss for VLM__
- Loss is calculated for a matrix of N image-text pairs in minibatch

$$\mathcal{L}_{VLM} = \mathbb{E}_{\sim minibatch}[\mathcal{L}_{CMPM}(v_{g}, t_{g})]$$

### Visual-fused Attributes Prediction:

- Just Masked Language Modeling but only masked the attribute keywords

- Don't need to use cross encoder like APTM, IRRA:
    - Just concatenate the global visual feature vector and text feature vector at masked position -> cross-modal representation vector for each masked position
    - Use a shared MLP to predict what the origin word at masked position

![image.png](docs/4.PLIP_files/33523f51-0cda-4d72-b2d5-84446da154ce.png)

### Semantic-fused Image Colorization (SIC) 

- 4 tensor output of vision encoder is concatenated together to use as input for Semantic-fused Image Colorization


![image.png](docs/4.PLIP_files/2bbe07d4-43bb-42cf-85dc-92ea43138c20.png)

![image.png](docs/4.PLIP_files/8a8e2cfc-3481-4cc5-9050-5aeb6b201be7.png)

- What in MSE (multimodal Sequeeze-Exciter) block? 
    - What is channel-wise attention?
        - Given a feature map $F$ with resolutioin C x H x W (C channel)
        - Try to generate a vector $s$ of size C that contains attention weight for each channel
    - For MSE block:
        - $F$: C1 x H x W --> Max Pooling --> vector $f$: size C1 --> concat with CLS text feature vector + Fully-connected layer --> a vector score $s$ with size C1 
        - $s$ --> broadcast --> $S$: C1 x H x W
        - Element-wise product $F$ and $S$ --> new output feature maps $Z$
        

- What is Deconv? 
    - Just a module use several transpose convolution layers to decode the feature maps $F$ into image $I_{restore}$

- Loss for SIC:
    - $\mathcal{D}$: L2 distance
$$\mathcal{L}_{\text {sic }}=\sum^N \mathcal{D}(I_{\text {origin }}, I_{\text {restore }} ) $$

# Training phase

## Dataset & Data Augmentation

## Implemention detail

- Train the model with 4 Geforce RTX 3090 GPUs for 70 epochs with batch size 512

## Evaluation result

# Inference phase

# Conclusion

## New points in this paper

## Pro

## Cons

## How to improve?

# Demo in notebook

Loss for pretraining is not shown in source code! Just vision encoder and text encoder are shown

## Set up

### Define path

### Import libries / local modules

### Load config

### Load model checkpoint

## Get and summary model

__Text encoder__

__Image Encoder__
