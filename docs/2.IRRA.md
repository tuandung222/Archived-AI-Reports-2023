#  Paper Information

- __Paper title__: *Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval*
- __Conference__: CVPR 2023
- __Link github__: https://github.com/anosorae/IRRA
- __Link paper__: https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Cross-Modal_Implicit_Relation_Reasoning_and_Aligning_for_Text-to-Image_Person_Retrieval_CVPR_2023_paper.pdf
- __Benchmark ranking__: Top $3^{rd}$ on CUHK-PEDES and RSTPReid benchmark

# Overview/Main contribution

![image.png](docs/2.IRRA_files/e5f9eaf2-2fbb-4f81-8898-4a578dddade3.png)

1. Global matching:
    - map images and texts global features into a joint embedding space
    - using matching losses only at the end of the network
    - __Drawbacks__: do not take advantage of the local features in the middle-level layers, low performance.
2. Local matching:
    - align on local features (body part in image and textual entities
    - better retrieval performance
    - __Drawbacks__: unavoidable noise and uncertainty problems because Image local feature vectors do not always have a 1-1 relationship with image feature vectors. resource-demanding because requires extracting and storing multiple local part representations of images and texts, computing pairwise similarity during inference
3. IRRA:
    - builds relations between visual and textual representations through self- and cross-attention mechanisms and masked language modeling (MLM) task
    - computes only one global image-text pair similarity score in the inference stage


## Main contribution

- Introduce a new loss named image-text __similarity distribution matching (SDM)__, used for image-text constrastive learning

- Utilize __CLIP__ - a *Vision-Language Pretraining (VLP)* model, as backbone
    - CLIP is a powerful backbone that was trained on a 400M pairs of image-text by contrastive learning method
    - CLIP is powerful enough to be used as a backbone -> don't need to create new large-scale dataset for general pretraining
    - Just focus on downstream task

        

- Use 3 objective function for finetuning on TBPR dataset:
    - SDM loss
    - MLM loss (similar to paper APTM)
    - ID loss (due to the available of ID label in all TBPR dataset)


- High results on three benchmarks: CUHK-PEDES, ICFG-PEDES and RSTPReid.

# Model Architecture / Method

__Re-draw model (done by Danh)__:

Link to whiteboard: https://miro.com/app/board/uXjVNVZpe9Y=/?share_link_id=620212020225

## Architecture

![image.png](docs/2.IRRA_files/e7567504-965e-4a78-bfad-562f80f3ac38.png)

![image.png](docs/2.IRRA_files/e7f4964b-85de-43bf-866c-22f03ba0be1d.png)

- Typical architecture: 3 encoder
    - A __vision encoder__: CLIP vision encoder
        - CLIP vision encoder has two version:
            - Version 1: Vision Transformer
            - Version 2: ResNet 
        - IRRA use Vision Transformer with CLIP pretrained parameter as default
        - Encode an image into a sequence of patch embedding, with extra __CLS__ token embedding at the start of the sequence
            - Behave like vision encoder of APTM
            - Use __CLS__ token as the global representation for the whole image
            - Need a linear layer to project the output __CLS__ embedding into a common space
    - A __language encoder__: a standard transformer-encoder with 12 encoder layers
        - Tokenize the text input into sequence of tokens by the lower-cased byte pair encoding (BPE) algorithm
        - Use extra __SOS__ token at the start of the sequence, __EOS__ token at the end of the sequence (NOT use CLS token)
        - __EOS__ token is used as global representation for the whole text (the role is similar to the CLS in BERT)
        - Need a linear layer to project the output __EOS__ token embedding to the common space
    - A cross encoder: 
        - Consist of a cross-attention module which use text representation as query, visual represention as key & value 
            - Place the Layer Normalize layer before the cross-attention layer (pre-norm)
        - Following cross-attention module is the standard transformer-encoder with 4 encoder layers
        - The output __EOS__ token embedding is used as global representation for image-text pair (behave like __CLS__ in APTM)
        

## Objective Functions

### Implicit Relation Reasoning

- Just Masked Lanugage Modeling (MLM) objective
    - The output embedding vectors from cross encoder for masked positions is used feed to a MLP for prediction
    - Consider as the problem of classification of the masked tokens.
    - Using Cross Entropy Loss


### Similarity Distribution Matching

![image.png](docs/2.IRRA_files/e018b1eb-50a0-459f-ad46-01ddd1d004d1.png)

- Let:
    - Minibatch is N pair of positive text-image, can be represented by NxN matrix
    - $f_i^v$ be the vector embedding of the $i^{th}$ image in minibatch
    - $f_j^t$ be the vector embedding of the $j^{th}$ text in minibatch
    - These vector embedding are output of encoders

- Similarity score between image and text usually calculate __in bi-directional__ manner:
    - image-to-text
    - text-to-image

- For image-to-text (loss is calculate per image)
    - Consider as image classfication problem, classify image belong to which text (class) in a list of text
    - Calculate similarity score by __cosine__ function ($sim$)
    - Smooth the score by using a __temperature hyperparameter__ $\tau$
        $$\operatorname{sim}\left(f_i^v, f_j^t\right) / \tau$$
    - Scale by __softmax__ for dimension of text
        \
        $\rightarrow p_{i,j} \in [0,1]$
    $$p_{i, j}=\frac{\exp \left(\operatorname{sim}\left(f_i^v, f_j^t\right) / \tau\right)}{\sum_{k=1}^N \exp \left(\operatorname{sim}\left(f_i^v, f_k^t\right) / \tau\right)}$$ 

- Can consider this is the probabilities of pair of image $i$ belong to text $j$ 
    - $[(p_{i,j})_{j=1}^{N}]$ is a __probability distribution vector__ of image $i$
    - Let $[(q_{i,j})_{j=1}^{N}]$ is the __ground-true probability distribution__ vector
    - In the other papers, just apply cross entropy loss and call the loss as *constrastive loss*!
    - In this paper, entropy loss is replaced by __KL Divergence__ loss, called SDM loss

- Similarity distribution matching (SDM) loss for a minibatch, image-to-text direction:
    - For each image i, calcute for the loss per each image
    - Then accumulate for all images in batch
    $$\mathcal{L}_{i 2 t}=K L\left(\mathbf{p}_{\mathbf{i}}|| \mathbf{q}_{\mathbf{i}}\right)=\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N p_{i, j} \log \left(\frac{p_{i, j}}{q_{i, j}+\epsilon}\right)$$

- For text to image, $\mathcal{L}_{t2i}$ is calculate in similar way
- Final SDM loss: $$\mathcal{L}_{SDM} = \frac {\mathcal{L}_{i2t} + \mathcal{L}_{t2i}}{2}$$

![image.png](docs/2.IRRA_files/13d3a4d7-7cb9-4a74-92ca-31a16b5c28d4.png)

### ID Loss

- Because, each image and caption has ID labels in datasets.
- Can consider as classification image and text to the corresponding classes.
- Using Cross Entropy Loss

## Final loss   


$$\mathcal{L} = \mathcal{L}_{SDM} + \mathcal{L}_{MLM} + \mathcal{L}_{ID}$$

# Training phase

## Dataset & Data Augmentation

- For image:
    - Random horizontally flipping
    - Random crop with padding
    - Random erasing

## Implemention detail

- For each layer of the cross encoder, the hidden size and number of heads are set to 512 and 8.
- All input images are resized to 384 × 128. 
- The maximum length of the textual token sequence L is set to 77
- Adam optimizer for 60 epochs with a
learning rate initialized to $10^{−5}$ and cosine learning rate
decay.
- Perform experiments on a single RTX3090 24GB GPU!

## Evaluation result

### Evaluate the efficiency of each loss

Why choose SDM ? Compare SDM to SMPM loss (a common constrastive loss)

![image.png](docs/2.IRRA_files/654d8e6a-2619-4099-87ec-ddb739902c0a.png)

# Inference phase

# Conclusion

## New points in this paper

## Pro

## Cons

## How to improve?

# Demo in notebook

## Set up

### Define path

### Import libries / local modules

### Load config for finetunning on RSTPReid

### Build model & load checkpoint

## View batch data

## Summary model

__Get name of model's module__

__Base model__

__ID classfier__

__MLP head__

__Cross Encoder__ (cross-attention module + 4 transformer encoder layers)

## Inference thourgh train model

## Print architecture image

## Loss

## Training loop

## Inference
