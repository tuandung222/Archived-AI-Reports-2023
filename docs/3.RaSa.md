#  Paper Information

- __Paper title__: *RaSa: Relation and Sensitivity Aware Representation Learning*
- __Date Published__: 23/5/2023
- __Conference__: IJCAI (rank A)
- __Link github__: https://github.com/Flame-Chasers/RaSa
- __Link paper__: https://arxiv.org/abs/2305.13653
- __Benmark ranking__: 
    - Top $1^{st}$ on 3 dataset benchmark for Text-based Person Search
    - Performance is better IRRA (CVPR 2023), APTM (AMM MM 2023) on three benchmarks

# Overview/Main contribution

## Recall baseline papers

Recall some papers before diving into RaSa

### Equivarient Contrastive Learning

- SOTA self-supervised learning (I-SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations
    - $f$ is a backbone that model need to learn
    - $g$ is any transformation in set of transformations $G$
    - SSL model try to learn $f$ that:
$$f(x) \sim f(g(x))$$
    - $f$ is called insensitive to $G$

- Some transformations, such as four-fold rotations, despite preserving semantic information, were shown to be harmful for image contrastive learning.

- Instead of being insensitive to rotations (invariance), training a neural network to predict them, i.e. to be sensitive to four-fold rotations, results in good image
representations

- Mathematical concepts of __equivariance__
    - Let $G$ be a group of transformations. For any $g \in G$ let $T_g(\boldsymbol{x})$ denote the function with which $g$ transforms an input image $\boldsymbol{x}$. For instance, if $G$ is the group of four-fold rotations then $T_g(\boldsymbol{x})$ rotates the image $\boldsymbol{x}$ by a multiple of $\pi / 2$. 
    - Let $f$ be the encoder network that computes feature representation, $f(\boldsymbol{x})$. I-SSL encourages the property of "invariance to $G$," which states $f\left(T_g(\boldsymbol{x})\right)=f(\boldsymbol{x})$, i.e. the output representation, $f(\boldsymbol{x})$, does not vary with $T_g$. 
    - __Equivariance__, a generalization of invariance, is defined as, $\forall \boldsymbol{x}: f\left(T_g(\boldsymbol{x})\right)=T_g^{\prime}(f(\boldsymbol{x}))$, where $T_g^{\prime}$ is a fixed transformation (i.e., without any parameters). 
    - Intuitively, equivariance encourages the feature representation to change in a well defined manner to the transformation applied to the input. Thus, invariance is a trivial instance of equivariance, where $T_g^{\prime}$ is the identity function, i.e. $T_g^{\prime}(f(\boldsymbol{x}))=f(\boldsymbol{x})$. 

- Propose new method equivariant Self Supervised Learning (E-SSL).
    - Adding a simple additional pre-training objective to encourages equivariance: predicting the transformations applied to the input

![image.png](docs/3.RaSa_files/8dff97eb-8a5d-4aa4-820a-e1f3c964cbb4.png)

![image.png](docs/3.RaSa_files/f40737e3-da89-4463-b75d-ca98da15923c.png)

## Main contribution

- Utilize __ALBEFF__ - a *Vision-Language Pretraining (VLP)* model, as backbone
    - ALBEF is a powerful backbone that was trained on a 14 pairs of image-text by contrastive learning (CL) and image-text matching (ITM).
    - ALBEFF is strong enough to be used as a backbone -> don't need to create new large-scale dataset for general pretraining
    - Just focus on downstream task

        

- Propose Sensitivity-aware Learning __(SA)__:
    - Just an improvement for Mask Language Modeling (MLM) objective 
    - Maintain a __Exponential Moving Average (EMA)__ version of model ($\theta$), called *monmentum* model $\hat{\theta}$ ($\hat{\theta}_o = {\theta}_o$)
    $$\hat{\theta}_t = m \cdot \hat{\theta}_{t-1} + (1-m) \cdot \theta_{t}$$
        - Monmentum model presents
a delayed and more stable version of the online model
        - No gradient decent for monmentum version, just use EMA formula
        - Use this monmentum version to do MLM task, may have some confusing in result
        - Given an image, and output text of MLM task done by monmentum model, make online model learn to predict which word was predict by monmentum version

- Relation-aware Learning __(SA)__:
    - Just an improvement for Image Text Matching (ITM) objective
    - Concern to the harmful and benefit of weak positive pairs
        - Sample a small probability of weak positive pair (hard input) to make model learn ITM better
        - Add an auxilary task: Given a positive pair, make the model classify whether it is strong or weak
    ![image.png](docs/3.RaSa_files/c54e4622-8f46-41a8-a11e-678fa789d622.png)

# Model Architecture / Method

__Architecture image (from paper)__

![image.png](docs/3.RaSa_files/fba00019-76d2-4aa6-a7c8-956f4492bce0.png)

- Question?
    - Architecture of RA module
    - Architecture of SA module
    - Monmentum Model?
    

##  Main component: 3 encoder

- 3 typical module: vision encoder, text encoder, cross encoder
    - Vision Encoder: utily the vision encoder of ALBEFF
        - Vision Transformer, 12 encoder layers
        - Dimension of output vectors: 768
    - Text Encoder: utily the text encoder  BERT of ALBEFF
        - First 6 (/12) layers of BERT 
        - Dimension of output vectors: 768
        - --> Behave like APTM
    - Cross Encoder: 
        - Last 6 layers of BERT

- Given an image-text pair $(I, T)$:
    - First feed the image $I$ into the image encoder to obtain a sequence of visual representations
$\{v_{cls}, v_1, \dotsc, v_M\}$
        - $v_{cls}$ is the global visual representation
    - Feed text $T$ to obtain a sequence of textual representations $\{t_{cls}, t_1, \dotsc, t_M\}$
        - $t_{cls}$ is the global visual representation
    - The visual and textual representations are then fed to the cross-modal encoder to obtain a sequence of multi-modal representations
        - $\{f_{cls}, f_1, \dotsc, t_M\}$, where $f_{cls}$ denotes the joint representation
of $I$ and $T$, 
        -  $f_{t}$ can be regarded as the joint representation of the image $I$ and the i-th token in the
text T. 
- The momentum version of each encoder is employed to obtain a sequence of momentum representations.

## Objective Functions

__Some notes before diving into each objective function__

- Definition of strong/weak positive pair: 
    - Let $(I_1, T_1)$ and $(I_2, T_2)$ are two pairs of (image, text description) belonged to person ID $P$
    - $(I_1, T_1)$, $(I_2, T_2)$ are *strong* positive pairs
    - $(I_1, T_2)$, $(I_2, T_1)$ are *weak* positive pairs

- Minibatch: N 4-element tuples, each tuple correspond to N pairs of (image-text caption):
    - For each strong positive pair pair $I, T$:
        - $I_{SA}, I_{RA}$ are two augmentation version of $I$
        - $T_{SA}$ is the strong positive text $T$
            - Paired with $I_{SA}$ to calculate loss for SA
        - $T_{RA}$ is the text that make positive pair with $I_1$ to calculate loss for RA
            - Sample the weak positive pair with small probability $p^{weak}$, strong positive pair with the  $1 - p^{weak}$
    - In summary:
        - Each element is a tuple of ($I_{SA}$,  $I_{SA}$, $T_{RA}$,  $T_{RA}$)
        - RA is just an improvement of Masked Language Modeling (MLM), SA is  just an improment of Image-Text Matching (ITM)
            - CL, ITM is two objective functions used to create backbone ALBEFF
            - RaSa just improve these objectives and add MLM -> 3 objectives 
        - Weak positive text is used only for RA, with a small $p$
        - SA use only strong positve text, not weak
        - Image used for each objective is the augmentation of ground-true image

```python
class ps_train_dataset(Dataset):
    def __len__(self):
        return len(self.pairs)

    def augment(self, caption, person):
        caption_aug = caption
        if np.random.random() < self.weak_pos_pair_probability: #p^weak
            caption_aug = np.random.choice(self.person2text[person], 1).item()
        if caption_aug == caption:
            replace = 0
        else:
            replace = 1
        return caption_aug, replace

    def __getitem__(self, index):
        image_path, caption, person = self.pairs[index]
        caption_aug, replace = self.augment(caption, person)
        image_path = os.path.join(self.image_root, image_path)
        image = Image.open(image_path).convert('RGB')
        image1 = self.transform(image)
        image2 = self.transform(image)
        caption1 = pre_caption(caption, self.max_words)
        caption2 = pre_caption(caption_aug, self.max_words)
        return image1, image2, caption1, caption2, person, replace
```

### Relation-aware Learning (RA)

- Given a minimbatch $\mathcal{B}$, consist of N 4-element tuples ($I_{SA}$,  $I_{SA}$, $T_{RA}$,  $T_{RA}$), each tuple correspond to a ground-true $(I, T)$ pair
    - From each $(I, T)$ <==> ($I_{SA}$,  $I_{SA}$, $T_{RA}$,  $T_{RA}$), sample 2 negative pairs, and only a positive pair
        - For image $I$, sample from the batch a hard negative text
        - For image $T$, sample from the batch a hard negative text
        - Hard negative text is choose by multinomial distribution, weight is the softmax-scaled of cosine score 
```python
        # select a negative image for each text
        image_neg_idx = torch.multinomial(weights_t2i, 1).flatten()
        image_embeds_neg = image_embeds[image_neg_idx]
        # select a negative text for each image
        text_neg_idx = torch.multinomial(weights_i2t, 1).flatten()
        text_embeds_neg = text_embeds[text_neg_idx]
        text_atts_neg = text2.attention_mask[text_neg_idx]
```


- How to sample positive pair for $(I, T)$?
    - First, there are 2 assumption related to weak positive pairs:
        - There exists __noise interference__ from weak positive pairs, which would hamper the representation learning. 
        - The weak positive pairs contain certain __valuable alignment information__ that can facilitate representation learning.
    <br>
    - Can utilize the weak positive to learning ITM better?

- Recall the ITM loss (use in almost papers like APTM), just BCE loss for binary classficatioin: 
    - $\mathcal{H}$: negative log of cross entropy 
    - $\phi$: ITM probability
    
$$L_{itm}(I, T)=\mathbb{E}_{(I, T) \sim p\left(I, T\right)}[ \mathcal{H}\left(y^{itm}, \phi^{itm}\left(I, T\right)\right)]$$

- How to sampling positive text $T_{RA}$ for image $I$
    - $T_{RA}$ has a change of $p^{weak}$ to be a weak positive text 
        - In the choice candidate list of weak postive texts, $T_{RA}$ is choose by uniform distribution
    - $T_{RA}$ has a change of 1 - $p^{weak}$ to be a strong positive text  
    - $p^{weak}$: hyperparameter, =0.1
    <br>
- For image $I$, use $I_{RA}$ to calculate $L_{p-itm}(I_{RA}, T_{RA})$, 
    <br>
    -->$L_{p-itm}$ is an improvement of original $L_{itm}(I, T)$


- Input for p-ITM head is the joint representation vector $f_{cls}$ of ($I$, $T$), that comes out of cross-encoder (behave like APTM)

- __Positive Relation Detection (PRD)__: auxilary task to detect the type of the positive pair $I_{RA}, T_{RA}$ is strong
or weak --> binary classification
    - PRD plays a role of a regularizer for p-ITM
    - This loss only applied for positive pair (not for negative pair)
    - Input for PRD head is also the joint representation $f_{cls}$
    $$L_{prd}(I_{RA}, T_{RA})=\mathbb{E}_{p\left(I_{RA}, T_{RA}\right)}[\mathcal{H}\left(y^{prd}, \phi^{prd}\left(I_{RA}, T_{RA}\right)\right)]$$

- __Final loss for RA__:
$$L_{RA} = L_{p-itm} + \lambda_1 L_{prd}$$

### Sensitivity-aware Learning (SA)

- Loss for MLM task:
    - Use $I_{SA}$, $T_{SA}$ (strong positive pair):
        $$L_{m l m}(I_{SA}, T_{SA})=\mathbb{E}_{p\left(I, T^{m s k}\right)} \mathcal{H}\left(y^{m l m}, \phi^{m l m}\left(I, T^{m s k}\right)\right)$$
    - Input for MLM head is the cross-model sequence of embedding $f_{i}$ of ($I_{SA}$, $T_{SA}$), get from cross-encoder
        - Apply MLM head for each token embedding at masked position
    - Stratregy for masking:
        - Step 1: choosing $p^{mlm}$ to masked ($p^{mlm}$ is hyperparameter, = 0.15)
            - Use Bernoulli distribution, the chance for a word to be choosen is $p^{mlm}$
        - Step 2: replace the choosen words by random token in whole vocabulary or __[MASK]__ token
            - First, use Bernoulli distribution, the chance for a word to be replace by __[MASK]__ is $0.8$
            - Then, use Bernoulli distribution again, the chance for a word to be replace by random token is $0.1$

- Adapt the MLM-based word replacement as the __sensitive transformation__ and propose a __Momentum-based Replaced Token Detection (m-RTD)__
    - Follow the *equivariant constrastive learning* paradism
    - __Momentum-based Replaced Token Detection (m-RTD)__: 
        - Use the result of MLM from the momentum model $T^{rep}$
        - Detect whether a token comes from the original textual description or the replacement --> just a binary classification problem
        - Input for m-RTD head is the cross-model sequence of embedding $f_{i}$ of $I$ and $T^{rep}$, get from cross-encoder
            - Apply m-RTD head for each token embedding at masked-and-replace position
        
        $$L_{m-r t d}=\mathbb{E}_{p\left(I, T^{r e p}\right)} \mathcal{H}\left(y^{m-r t d}, \phi^{m-r t d}\left(I, T^{r e p}\right)\right)$$
        
    - Stratregy for masking text before feed into monmentum encoder: like the stratregy for MLM, but replace hyperparameter $p^{mlm}$ by $p^{m-rtd} = 0.3$

![image.png](docs/3.RaSa_files/e39592a9-942a-4b05-aea4-648c5eeff7fb.png)

### Constrastive Learning

![image.png](docs/3.RaSa_files/76605a2c-172d-4132-a3a3-cca3ed241942.png)

- Recall the InfoNCE loss, 
    - From paper "Momentum Contrast for Unsupervised Visual Representation Learning
", CPVR 2020

    - Formula: 
        - $\tau$ is a learnable temperature parameter
        - $Q$ is a queue with large size >> size of minibatch
        - $x$ is the query feature coming from the online encoder, while $x_{+}$ is the positive key feature comes from monmentum version
    $$
L_{n c e}\left(x, x_{+}, Q\right)=\mathbb{E}_{p\left(x, x_{+}\right)}\left[-\log \frac{\exp \left(sim\left(x, x_{+}\right) / \tau\right)}{\sum_{x_i \in Q} \exp \left(sim\left(x, x_i\right) / \tau\right)}\right]
$$
        

- Loss for Image-Text constrastive learning
    - Called inter-modal constrastive loss
    - queue_size: 65536 >> size of minibatch

$$
L_{i t c}=\left[L_{n c e}\left(v_{c l s}^{\prime}, \hat{t}_{c l s}^{\prime}, \hat{Q}_t\right)+L_{n c e}\left(t_{c l s}^{\prime}, \hat{v}_{c l s}^{\prime}, \hat{Q}_v\right)\right] / 2
$$

- Besides, use intra-modal contrastive learning (IMC) for image-image, text-text. 
    - Make the representations of the same person are supposed to stay closer than those of different persons within each modality. 
    - IMC loss is formulated as:
$$
L_{i m c}=\left[L_{n c e}\left(v_{c l s}^{\prime}, \hat{v}_{c l s}^{\prime}, \hat{Q}_v\right)+L_{n c e}\left(t_{c l s}^{\prime}, \hat{t}_{c l s}^{\prime}, \hat{Q}_t\right)\right] / 2
$$

- Taken together, the overall loss for $\mathrm{CL}$ is:
$$
L_{c l}=\left(L_{i t c}+L_{i m c}\right) / 2 .
$$

__Final Loss__
- Formulate the joint optimization objective as:
$$
L_{final}=L_{r a}+L_{s a}+\lambda_3 L_{c l},
$$

# Training phase

## Dataset & Data Augmentation

Only Random Horizontal Flip (insensitive transformation) is used for augment image

## Implemention detail

## Evaluation result

__Evaluate the efficent of losses on CUHK-PEDES__

![image.png](docs/3.RaSa_files/d21f5b79-cba6-47ca-a80a-afa9b4042b73.png)

# Inference phase

- First calculate each pair’s similarity $sim(t_{cls}, v_{cls})$ via the unimodal
encoders
- Select the first 128 images with the highest similarities to send them to the cross-modal encoder
- Compute the p-ITM matching probabilities for ranking.

# Conclusion

## New points in this paper

## Pro

## Cons

## How to improve?

# Demo in notebook

## Set up

### Define path

### Import libries / local modules

### Load config

### Load model checkpoint

## Get and summary model
