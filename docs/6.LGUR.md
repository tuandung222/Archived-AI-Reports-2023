#  Paper Information

- __Paper title__: Learning Granularity-Unified Representations for Text-to-Image Person Re-identification
- __Date Published__: 16/7/2022
- __Conference__: ACM MM 2022 (rank A)
- __Link github__: https://github.com/ZhiyinShao-H/LGUR
- The preferred choice for using as TBPR baseline to evaluate the transferability of backbone, such as: PLIP, HAP, UniPT

# Overview/Main contribution

## Main contribution

- Design two modules for learning relation between text and image (replace for cross encoder with high cost of computation)
    - __Dictionary-based Granularity Alignment (DGA)__: for reconstruct texual representation $T$ and visual represenatation $V$ in a common space $D$
        - Let $\mathcal{T}_D(Q, K, V)$ as Transformer Encoder block that replace self-attention by cross-attention 
    $$V_{re} = \mathcal{T}_V(V, D, D)$$ 
    <br>
    $$ T_{re} = \mathcal{T}_T(T, D, D) $$
        - D is special representation called atom, that is modality-independent
        - D is used as key and value, for learning general representation of text/image from atom, not for learning fine-grained details
    - __Prototype-based Granularity Unification (PGU)__:
        - Learning fine-grained representation of text/image from the common reconstruction space $D$
        - $P$ is special query called prototype, that is for learning the fine-grained detailed represenation
        - $P$ is used as the query, while $V$ or $T$ is used as the key and value
    $$V_{pgu} = \mathcal{P}[\mathcal{T_P}(P, V_{re}, V_{re})]$$ 
    <br>
    $$ T_{pgu} = \mathcal{P}[\mathcal{T_P}(P, T_{re}, T_{re})] $$
    
        - Output representation of this module is also in space $D$
    
    - Number of vectors in $D, P$ is << number of vectors in $V, T$ --> lower cost of computation compared to image-text cross-attention in APTM, IRRA,...

![image.png](docs/6.LGUR_files/d7b7d9cb-6c36-4075-8d3f-4e3ec1f22762.png)

- Lightweight architecture for textual / visual encoders

# Model Architecture / Method

## Architecture

__Image/Link to image: ...__

### Vision Encoder

![image.png](docs/6.LGUR_files/2de95f83-25f9-4f9f-a5f0-28456556eb7e.png)

- Two choice for vision encoder:
    - ResNet50: 
        - Encode image $I$ into a tensor H x W x C
    - DeiT Transformer: DeiT is created by knowledge distilation method, with the teacher is Vision Transformer --> DeiT is lightweight compared to its teacher
        - Preprocess image $I$ into L = H x W patches
        - Encode sequence of patches into a sequence of vectors: L x d (behave like Vision Transformer)
- Consider output of vision encoder is a tensor H x W x C or a sequence of vector L x C

### Text Encoder

- A learnable bi-directional LSTM following the frozen BERT, which encode the text $T$ into a sequence of vectors L x d 

![image.png](docs/6.LGUR_files/de36bb74-3a08-4294-890f-fa085ef5f8eb.png)

### DGA module

- Let:
    - $\mathbf{T} \in \mathbb{R}^{L_{T}\text{ x }d_{T}}, \mathbf{V} \in \mathbb{R}^{H_V \text{ x }  W_V \text{ x } d_{V}}$ be the output of encoders
    --> Input of DGA module
    - __Multi-modality shared dictionary__ (MSD): 
        - $\textbf{D} \in \mathbb{R}^{s\text{ x }d}$, $s$ is the numbers of atom, $d$ is the dimension of vector
        - $\textbf{D}$ is random initilized, considered as a sequence of atoms
        
        - For default, $s = 400, d = 384$

- DGA is combined of 2 modules: Visual Feature Recontruction (VFR) and Text Feature Reconstruction (TFR)

![image.png](docs/6.LGUR_files/c93961a7-71d5-4479-8f15-dadf41d4372d.png)

- What is $\textbf{MHA}$ ?: just a transformer encoder that replace self-attention by cross-attention
$$ğ‘€ğ»ğ´(Q, K, V) = ğ¹ğ¹ğ‘ (ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ (Q, K, V))$$

__Text Feature Reconstruction__

- For text represention: input is $\mathbf{T}$, output is $\textbf{T}_{re}, \textbf{T}$
    $$\textbf{T}_{re} = MHA(\textbf{T},\textbf{D}, \textbf{D})$$
    ![image.png](docs/6.LGUR_files/6711bfd1-c506-4052-97a4-8002f1ecd5ab.png)

__Visual Feature Reconstruction__

- For visual representation:
    - Input is $\mathbf{V}$
    - Output is $\textbf{V}_{re}, \textbf{V}_{g}$
        - $\textbf{V}_{re}$ is represenatation of $\textbf{V}$ in new reconstruction space $\mathcal{D}$, guided by $\textbf{D}$
        - $\textbf{V}_{g}$ is the new represenatation of $\textbf{V}$ in text space $\mathcal{T}$, guided by $\textbf{T}$
        
    <br>
    
    - A mask of score $M$ with same resolution as $\mathbf{V}$, channel = 1, is generated for spatial attention mechanism 
        - Use 1x1 convolution with number filter = 1 and sigmoid activation 
    ![image.png](docs/6.LGUR_files/18d5951d-01c2-4e43-b9e4-b01724072629.png)
    
    - Can consider $\mathbf{V}$ as a sequence of vectors L x C, or a tensor of feature maps H x W x C (with L = H x W)
    $$\mathbf{V}_{re} = MHA(\mathbf{V},\mathbf{D}, \mathbf{D}) \odot M$$ 
    <br>
    $$\mathbf{V}_{g} = MHA(\mathbf{V},\mathbf{T}, \mathbf{T}) \odot M$$ 


- In summary, output of DGA module is $\textbf{V}_{re}, \textbf{V}_{g}, \textbf{T}_{re}, \textbf{T} = DGA(\textbf{T}, \textbf{V})$:
    - $\textbf{V}_{re}, \textbf{V}_{g} = VFR(\textbf{V})$
    
    - $\textbf{T}_{re}, \textbf{T} = TFR(\textbf{T})$
    
    - Can consider:
        - $\textbf{V}_{re}, \textbf{T}_{re}$ as the representation of text/image in common reconstruction space $\mathcal{D}$
        - $\textbf{V}_{g}, \textbf{T}$ as the representation of text/image in text space $\mathcal{T}$

### PGU module

- Input of PGU is $\textbf{V}_{re}, \textbf{V}_{g}, \textbf{T}_{re}, \textbf{T}$, consider each as sequence of vectors $L \text{ x } d_{in}$

- $\textbf{P} \in \mathbb{R}^{K\text{ x }d}$, $K$ is the numbers of protypes, $d$ is the dimension of vector (= $d_{in}$)
    - Consider $P$ as $K$ sequence of query, each sequence $i$ contains only 1 vector $\mathrm{p}_i$

- For convenient, let $\mathbf{F}$ is the representation for each of $\{ \textbf{V}_{re}, \textbf{V}_{g}, \textbf{T}_{re}, \textbf{T} \}$
    - Then:
    $$\widetilde{\mathbf{F}}=P G U(\mathbf{P}, \mathbf{F}) =\operatorname{Concat}\left(f_1\left(\mathrm{p}_1, \mathbf{F}\right), \ldots, f_K\left(\mathrm{p}_K, \mathbf{F}\right)\right) $$
    
    - What is $f_i$:
        - $ğ‘€ğ»ğ´_2(Q, K, V) = ğ¹ğ¹ğ‘ (ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ (Q, K, V))$ - just a transformer encoder block
    
    
        - $f_i\left (\mathbf{p}_i, \mathbf{F}\right) =\mathbf{W}_i\left(M H A_2\left(\mathrm{p}_i, \mathbf{F}, \mathbf{F}\right)\right)$
    
    
        - $\mathbf{W}_i \in \mathbb{R}^{d_{out} \text{ x }d_{in}}$  is an owned projection matrix for each vector query $f_i$
        
        - $\mathrm{p}_i$ is sequence of query with length 1!
        
        - $\widetilde{\mathbf{F}} \in \mathbb{R}^{K \text{ x }d_{out}}$

- In summary, output of PGU module is $\widetilde{\textbf{V}}_{re}, \widetilde{\textbf{V}}_{g}, \widetilde{\textbf{T}}_{re}, \widetilde{\textbf{T}}$:
    - $\widetilde{\mathbf{V}_{re}}=P G U(\mathbf{P}, \mathbf{V_{re}})$,...
    - Consider each of those as a sequence $\widetilde{\mathrm{F}}$ of vectors with length $K$

![image.png](docs/6.LGUR_files/19a56ddb-1217-4e26-bb80-32cc5eb72500.png)

## Objective Functions

Each minibatch contain N pair of image-text

__ID loss__

ID loss is applied for each sequences $\widetilde{\mathrm{F}}$ of $K$ vectors

$$
L_{I D}(\widetilde{\mathrm{F}})=\frac{1}{K} \sum_{k=1}-\mathrm{y} \odot \log \left(\hat{\mathrm{y}}_k\right)
$$

__Ranking loss__

For two features $\widetilde{\mathrm{F}}_1$ and $\widetilde{\mathrm{F}}_2$ from one matched image-text pair, the ranking loss is formulated as follows:
$$
\begin{aligned}
L_{R K}\left(\widetilde{\mathrm{F}}_1, \widetilde{\mathrm{F}}_2\right) & =\max \left(\alpha-S\left(\widetilde{\mathrm{F}}_1, \widetilde{\mathrm{F}}_2^{+}\right)+S\left(\widetilde{\mathrm{F}}_1, \widetilde{\mathrm{F}}_2^{-}\right), 0\right) \\
& +\max \left(\alpha-S\left(\widetilde{\mathrm{F}}_2, \widetilde{\mathrm{F}}_1^{+}\right)+S\left(\widetilde{\mathrm{F}}_2, \widetilde{\mathrm{F}}_1^{-}\right), 0\right)
\end{aligned}
$$
- Where :
    - $\widetilde{\mathrm{F}}_1^{+} / \widetilde{\mathrm{F}}_2^{+}$and $\widetilde{\mathrm{F}}_1^{-} / \widetilde{\mathrm{F}}_2^{-}$are one positive sample and one hard negative sample of $\widetilde{F}_1 / \widetilde{F}_2$ in a mini-batch. 
    - In addition, $\alpha$ is a margin hyper-parameter, while $S$ denotes the cosine similarity metric.

__Final Loss__

$$
\begin{aligned}
L_M & =L_{I D}\left(\widetilde{\mathrm{T}}_{r e}\right)+L_{I D}\left(\widetilde{\mathrm{V}}_{r e}\right)+L_{I D}(\widetilde{\mathrm{T}})+L_{I D}\left(\widetilde{\mathrm{V}}_g\right) \\
& +L_{R K}\left(\widetilde{\mathrm{T}}_{r e}, \widetilde{\mathrm{V}}_{r e}\right)+L_{R K}\left(\widetilde{\mathrm{T}}, \widetilde{\mathrm{V}}_g\right) .
\end{aligned}
$$

- Apply ranking loss to pull the reconstructed features closer to the guidance features when they refer to the same person:
$$
L_G=L_{R K}\left(\widetilde{\mathrm{T}}_{r e}, \widetilde{\mathrm{T}}\right)+L_{R K}\left(\widetilde{\mathrm{V}}_{r e}, \widetilde{\mathrm{V}}_g\right) .
$$

- The overall loss function can thus be expressed as:
$$
L=L_M+L_G .
$$

# Training phase

## Implemention detail

- Resize all images to 384 Ã— 128 pixels and use only random horizontal flipping as the data augmentation. 
- The feature dimension ğ‘‘ input of PGU for both the image and text to 384. 
- The feaeture dimension ğ‘‘ output of PGU is 512. 
- The dictionary size ğ‘  is 400 and the margin ğ›¼ is set to 0.3. 
- The number of shared prototypes ğ¾ is set to 6. 
- Use Adam optimizer, batch size is 64, and the number of epochs is 60.

## Evaluation result

# Inference phase

- Just use $\widetilde{\mathrm{T}}_{re}, \widetilde{\mathrm{V}}_{re}$ (representation of text, image after PGU module in common reconstruction space $\mathcal{D}$) for calculate cosine simlarity

# Conclusion

## New points in this paper

## Pro

## Cons

## How to improve?

# Demo in notebook

## Set up

### Define path

### Import libries / local modules

### Load config

### Load model checkpoint

## Get and summary model
