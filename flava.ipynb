{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tên paper: __FLAVA: A Foundational Language And Vision Alignment Model__\n",
    "- Conference: CVPR 2022\n",
    "- Author: Facebook AI Research (FAIR)\n",
    "- Link: https://flava-model.github.io/\n",
    "- FLAVA là một backbone phổ biến, được re-implement bởi các framework: Huggingface Transformers, TorchMultimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Điểm mới (Main contribution)\n",
    "- Tạo ra một kiểu pretraining mới: kết hợp __Unimodal Pretraining__ và __Cross-modal Pretraining__ \n",
    "    - Ý nghĩa:\n",
    "        - Tận dụng được nguồn dataset hỗn tạp: image - text pair, image + label, text + label\n",
    "        - Tạo ra một backbone có thể transfer cho các *cross-modal task* như Visual Question Answering, Image Captioning, Text-to-Image Retrieval,... và các *unimodal task* như Image Classification, Text Classification, ...\n",
    "    - Tóm tắt sơ lược: phase 1 Unimodal Pretraining; phase2  kết hợp Unimodal và Cross-modal Pretraining\n",
    "        - After unimodal pretraining of the image and text encoders, we continue training the entire FLAVA model jointly on the three types of datasets with round-robin sampling.\n",
    "        - In each training iteration, we choose one of the datasets according to a sampling ratio that we determine empirically and obtain a batch of samples.\n",
    "        - Then, depending on the dataset type, we apply unimodal MIM on image data, unimodal MLM on text data, or the multimodal losses (contrastive, MMM, and ITM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- image path: imgs/FLAVA_1.png -->\n",
    "![FLAVA_1](imgs/FLAVA_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đề xuất __Masked Multimodal Modeling (MMM) loss__, tóm tắt lại như sau: \n",
    "    - Từ một ảnh thông qua bộ image encoder được mã hóa thành sequence các vector embedding $S_I$\n",
    "    - Từ một đoạn text thông qua bộ text encoder được mã hóa thành sequence các vector embedding $S_T$\n",
    "    - Cho concat 2 sequence $S_I$ và $S_T$ thành một sequence $S_{M}$ (__multimodal sequence__). Sau đó cho che một số phần tử của $S_{M}$ thành token [MASK] (tương tự như *BERT* (với text) và *BEiT* (với image)).\n",
    "    - Masked sequence này sẽ được encode bởi multimodal encoder (còn gọi là cross encoder, fusion encoder). Phần vector embedding output ứng với các token [MASK] sẽ được feed qua một module để dự đoán lại các token bị che.\n",
    "    - Với các masked text token, cần dự đoán token ứng với từ nào trong bộ word dictionary, còn với các image token bị che là token nào trong bộ image codebook. (Chi tiết task che hình đi đoán lại và khái niệm codebook sẽ được phân tích trong bài viết về BEiT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FLAVA_2](imgs/FLAVA_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phân tích mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Kiến trúc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiến trúc kinh điển của một Vision-Language model bao gồm 3 thành phần chính: image encoder, text encoder, và multimodal encoder. FLAVA cũng không phải ngoại lệ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Image Encoder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kiến trúc Vision Transformer (ViT), cụ thể là ViT-B/16.\n",
    "- Phía sau Image Encoder là một lớp linear projection để điều chỉnh số chiều của vector embedding.\n",
    "- Vector embedding ứng với __[CLS]__ token được dùng làm global representation của ảnh. Biểu diễn của token này được dùng để tính loss contrastive. -> mô típ quen thuộc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text Encoder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dùng kiến trúc ViT-B/16 giống như Image Encoder\n",
    "    - Câu hỏi: ViT khác BERT chỗ nào? \n",
    "        - Các vision backbone dựa trên transformer thường để lớp Layer Normalization ngay trước module Self-attention\n",
    "        - Còn BERT để lớp Layer Normalization ngay sau module Self-attention.\n",
    "    - Image Encoder giống Text Encoder về mặt kiến trúc, chỉ khác nhau ở module embedding: khâu mã hóa input thành sequence của các vector embedding. Nói chung, Text Encoder kiến trúc là ViT-B/16, nhưng module *text embedding* lấy từ BERT.\n",
    "    - Việc dùng kiến trúc ViT-B/16 tạo ra sự đồng nhất về mặt kiến trúc giữa Image Encoder và Text Encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector embedding ứng với __[CLS]__ token được dùng làm global representation của toàn đoạn text. Biểu diễn của token này được dùng để tính loss contrastive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Multimodal Encoder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kiến trúc là một khối Transformer Encoder. Paper Flava gọi khối này là multimodal encoder, một số paper khác gọi là cross encoder hoặc fusion encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multimodal Encoder chỉ dùng self-attention mà không dùng cơ chế cross-attention như bộ Cross Encoder của APTM, ALBEF. Yếu tố 'fusion' trong multimodal encoder của Flava được thể hiện thông qua việc dùng multimodal sequence $S_M$ (concat của $S_I$ và $S_T$) để làm input đầu vào."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chú ý:\n",
    "    - Một ảnh $I$ thông qua image encoder được mã hóa thành một sequence các vector embedding, tạm gọi là $E_I$. Sau đó $E_I$ cần đi qua bộ vision projection để điều chỉnh số chiều, từ đó thu được sequence mới là $S_I$.\n",
    "    - Tương tự với đoạn text $T$, cần thông qua text encoder và bộ text projection để thu được sequence $S_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trong bài báo tóm tắt về Vision-Language model của Microsoft Research, kiểu kiến trúc này được gọi là __merge-attention__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Co-attetnion_Merge-attetion.png -->\n",
    "![Co-attetnion_Merge-attetion](imgs/Co-attetnion_Merge-attetion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Unimodal Pretraining Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quá trình pretraining của FLAVA sẽ qua hai phase: phase 1 là Unimodal Pretraining, phase 2 là Cross-modal Pretraining. Phần này sẽ phân tích về phase 1.\n",
    "- Tổng quan phase 1\n",
    "    - Train text encoder trên tập dữ liệu chỉ có text, train image encoder trên tập dữ liệu chỉ có image.\n",
    "    - Dùng phương pháp self-supervised, không cần label. \n",
    "    - Pretext task __Mask Language Modeling__ (ý tưởng lấy từ BERT), được dùng làm objective để pretrain cho text encoder.\n",
    "        - We apply a masked language modeling loss on top of the text encoder to pretrain on stand-alone text datasets. A fraction (15%) of the text tokens are masked in the input, and reconstructed from the other tokens using a classifier over the unimodal text hidden states output.\n",
    "    - Pretext task __Mask Image Modeling__ (ý tưởng lấy từ BEiT), được dùng làm objective để pretrain cho image encoder.\n",
    "        - On unimodal image datasets, we mask a set of image patches following the rectangular block-wise masking in BEiT and reconstruct them from other image patches. The input image is first tokenized using a pretrained **dVAE tokenizer**. Then a classifier is applied on the image encoder outputs to predict the dVAE tokens of the masked patches.\n",
    "        - Ý tưởng này có thể tóm lại như sau: Trước tiên cần xây dựng một bộ từ điển cho ảnh gọi là visual codebook (kích thước cố định) và một bộ tokenizer. Với một ảnh kích thước bất kì, đều có thể tokenize thành các visual token rời rạc trong bộ từ điển. Các token trong bộ từ điển là một số điểm trong latent space, mỗi điểm được biểu diễn bằng 1 vector.\n",
    "        - Câu hỏi đặt ra?\n",
    "            - Phương pháp tạo ra bộ từ điển và bộ tokenizer?\n",
    "            - Cơ chế mask image của BEiT?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nguồn dataset cho unimodal pretraining:\n",
    "     - Text dataset: CCNews và BookCorpus\n",
    "     - Image dataset: ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Một điểm đáng chú ý: đối với Image Pretraining, phương pháp self-supervised __DINO__ được các tác giả của FLAVA chỉ ra có performance tốt hơn so với phương pháp dùng Mask Image Modeling như các backbone MAE, BEiT. Nhưng do tính dễ mở rộng và tính đơn giản, MIM task được dùng kết hợp trong quá trình Cross-modal Pretraining ở phase 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Review lại về BEiT__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Language Modeling là một pretext task kinh điển, không có vấn đề để bàn. Do đó, phần này chỉ tập trung review lại BEiT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BEiT](imgs/BEiT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overview of BEIT pre-training. Before pre-training, we learn an “image tokenizer” via\n",
    "autoencoding-style reconstruction, where an image is tokenized into __discrete visual tokens__ according\n",
    "to the __learned vocabulary__. During pre-training, each image has two views, i.e., image patches, and visual tokens. We randomly mask some proportion of image patches (gray patches in the figure) and replace them with a special mask embedding. Then the patches are fed to a backbone vision\n",
    "Transformer. The pre-training task aims at predicting the visual tokens of the original image based\n",
    "on the encoding vectors of the corrupted image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
