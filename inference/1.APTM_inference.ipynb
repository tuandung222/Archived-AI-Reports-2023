{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c679229-3b72-46a7-9212-63848a795cee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aea7ba-64e1-4bed-bf78-8a71999fba19",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use APTM ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a26250-48e0-4b6c-b454-43fd02b83cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bd3fed-9b8f-43dc-9f5a-da2397b1ac83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "ROOT_PATH = Path('../../paper_clones/APTM').resolve()\n",
    "sys.path.append(str(ROOT_PATH))\n",
    "IMAGE_PATH = Path('../../DATASET/CUHK-PEDES/imgs').resolve()\n",
    "sys.path.append(str(IMAGE_PATH))\n",
    "MODEL_PATH = ROOT_PATH/'MODEL'/'ft_cuhk'/'checkpoint_best.pth'\n",
    "ANNO_PATH = ROOT_PATH/'data/finetune'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcfda8f-6b3f-4e9a-9285-dc3ffaa8c183",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a395a0a-2b03-4d1b-985c-dd6b6b36cb89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/home/jovyan/workspace/BA-PRE_THESIS/REPORT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 11:48:55.585492: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available())\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch.nn.functional as F\n",
    "from easydict import EasyDict as edict\n",
    "from torchinfo import summary\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "from prettytable import PrettyTable\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import PIL\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "#### LOCAL MODULES\n",
    "from models import aptm, bert, model_retrieval, swin_transformer, tokenization_bert\n",
    "from models.model_retrieval import APTM_Retrieval\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "import utils\n",
    "from dataset import create_dataset, create_sampler, create_loader\n",
    "from dataset.re_dataset import TextMaskingGenerator\n",
    "from scheduler import create_scheduler\n",
    "from optim import create_optimizer\n",
    "from trains import train, train_attr\n",
    "from train_pa100ks import train_pa100k, train_pa100k_only_img_classifier\n",
    "from reTools import evaluation, mAP\n",
    "from reTools import evaluation_attr, itm_eval_attr\n",
    "from reTools import evaluation_attr_only_img_classifier, itm_eval_attr_only_img_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33cb101a-81fd-4760-a8d5-c351d12f0b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FOR VECTOR DATABASE\n",
    "import pymilvus\n",
    "from pymilvus import MilvusClient, Collection\n",
    "from pymilvus import connections, utility\n",
    "from pymilvus import Collection, DataType, FieldSchema, CollectionSchema\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8cf09f-7cc1-4a65-b4bc-c068c78fe68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ZILLIZ SERVICE\n",
    "MILVUS_URI = \"https://in01-e512c229cfb7739.aws-us-west-2.vectordb.zillizcloud.com:19531\"\n",
    "TOKEN = \"db_admin:HelloKitty!\"\n",
    "USER = \"db_admin\"\n",
    "PASSWORD = \"HelloKitty!\"\n",
    "pymilvus.connections.connect(\n",
    "    \"default\",\n",
    "    uri=MILVUS_URI,\n",
    "    user=USER,\n",
    "    password=PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f412e2e-ea30-46f3-b732-36a5d3b7619f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load model config for finetune on CUHK-PEDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab372327-9a64-4f6f-b597-e73c7b49cc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_reader = lambda file: yaml.load(open(file, 'r'), Loader=yaml.Loader)\n",
    "config_path = ROOT_PATH / \"configs\"\n",
    "retrieval_cuhk_config = config_reader(config_path / \"Retrieval_cuhk.yaml\")\n",
    "retrieval_pa100k_config = config_reader(config_path / \"Retrieval_pa100k.yaml\")\n",
    "config = retrieval_cuhk_config\n",
    "config['vision_config'] = config_path / 'config_swinB_384.json'\n",
    "config['text_encoder'] = 'bert-base-uncased'\n",
    "config['text_config'] = config_path / 'config_bert.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56bfe4-33f5-4b98-813a-79a0d7a8f8d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load dataset config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a8f7c6-1d1f-40f7-aa0d-e4439b7deee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['image_root'] = IMAGE_PATH\n",
    "config['test_file'] = ANNO_PATH/'cuhk_test.json'\n",
    "config['val_file'] = ANNO_PATH/'cuhk_val.json'\n",
    "config['train_file'] = [ANNO_PATH/'cuhk_train.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02447a9-b792-455b-a01a-47ca397ec83e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define class APTM_Inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00f5eee-2e5e-4458-aa74-8cc76db83a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class APTMInference(APTM_Retrieval):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config_dict = config\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.cuhk_norm = transforms.Normalize((0.38901278, 0.3651612, 0.34836376), (0.24344306, 0.23738699, 0.23368555))\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize((config['h'], config['w']), interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "            self.cuhk_norm,\n",
    "        ])\n",
    "\n",
    "    def encode_image(self, image, device='cuda'):\n",
    "        self.vision_encoder.to(device)\n",
    "        self.vision_proj.to(device)\n",
    "        # output embedding sequence, dim = 1024\n",
    "        image_embs_sequence = self.vision_encoder(image.to(device))\n",
    "        # global feature from CLS\n",
    "        image_cls_feature = self.vision_proj(image_embs_sequence[:, 0, :])\n",
    "        image_cls_feature = F.normalize(image_cls_feature, dim=-1)\n",
    "        \n",
    "        # self.vision_encoder.to('cpu')\n",
    "        # self.vision_proj.to('cpu')\n",
    "        \n",
    "        return {\n",
    "            \"sequence\": image_embs_sequence,\n",
    "            \"cls_feature\": image_cls_feature\n",
    "        }\n",
    "\n",
    "    def tokenize_text(self, texts):\n",
    "        text_model_input = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.config_dict[\"max_tokens\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return text_model_input\n",
    "\n",
    "    def encode_text(self, text: str, device='cuda'):\n",
    "        self.text_encoder.bert.to(device)\n",
    "        self.text_proj.to(device)\n",
    "        # tokenization\n",
    "        text_model_input = self.tokenize_text(text).to(device)\n",
    "        text_attention_mask = text_model_input.attention_mask\n",
    "        # output embedding sequence, dim = 768\n",
    "        text_embs_sequence = self.text_encoder.bert(\n",
    "            text_model_input.input_ids,\n",
    "            attention_mask=text_model_input.attention_mask,\n",
    "            return_dict=True,\n",
    "            mode=\"text\",\n",
    "        ).last_hidden_state\n",
    "        # cls embedding\n",
    "        text_cls_feature = self.text_proj(text_embs_sequence[:, 0, :])\n",
    "        text_cls_feature = F.normalize(text_cls_feature, dim=-1)\n",
    "        ########################## Get out of GPU\n",
    "        # self.text_encoder.bert.to('cpu')\n",
    "        self.text_proj.to('cpu')\n",
    "        return {\n",
    "            \"sequence\": text_embs_sequence,\n",
    "            \"attention_mask\": text_attention_mask,\n",
    "            \"cls_feature\": text_cls_feature,\n",
    "        }\n",
    "\n",
    "    def encode_cross_modal(self, image_sequence, text_sequence, text_attention_mask, device='cuda'):\n",
    "        # image attention mask\n",
    "        image_attention_mask = torch.ones(\n",
    "            image_sequence.size()[:-1], dtype=torch.long\n",
    "        ).to(device)\n",
    "        # output embedding\n",
    "        cross_sequence = self.text_encoder.bert.to(device)(\n",
    "            encoder_embeds=text_sequence,\n",
    "            attention_mask=text_attention_mask,\n",
    "            encoder_hidden_states=image_sequence,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True,\n",
    "            mode=\"fusion\",\n",
    "        ).last_hidden_state\n",
    "        # cls token\n",
    "        cross_cls_feature = cross_sequence[:, 0, :]\n",
    "\n",
    "        # self.text_encoder.bert.to('cpu')\n",
    "        return {\"sequence\": cross_sequence, \"cls_feature\": cross_cls_feature}\n",
    "\n",
    "    def check_matching(self, cross_cls_feature, device='cuda'):\n",
    "        logits = self.itm_head.to(device)(cross_cls_feature.to(device))\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        class_res = probs.argmax(dim=-1)\n",
    "        # self.itm_head.to('cpu')\n",
    "        return {'logits': logits, 'probs': probs, 'class': class_res}\n",
    "    \n",
    "    def read_image(self, img_path):\n",
    "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "        return self.test_transform(img)\n",
    "\n",
    "#     def end2end_filter(self, image_sequence_dict, text_dict):\n",
    "#         image_infer_result = self.encode_image(image)\n",
    "#         text_infer_result = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116b2761-ca99-4520-bef6-1eece30b3eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_swin\n",
      "### Loading pretrained vision encoder\n",
      "### Loading pretrained text encoder\n",
      "load checkpoint from /home/jovyan/workspace/BA-PRE_THESIS/paper_clones/APTM/MODEL/ft_cuhk/checkpoint_best.pth\n",
      "missing_keys:  []\n",
      "vision_encoder missing_keys:  []\n",
      "unexpected_keys:  []\n"
     ]
    }
   ],
   "source": [
    "model = APTMInference(retrieval_cuhk_config)\n",
    "model.load_pretrained(\n",
    "    MODEL_PATH, config, is_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcfe233-fb4c-4e5d-a12a-749e839203f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b440ab28-3937-4968-9e9d-10c29027ed6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['batch_size_test'] = 16\n",
    "train_dataset, val_dataset, test_dataset = create_dataset('re_cuhk', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0239f85e-10b8-4b6e-95bf-3d976b794934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3074, 193)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## samplers = None: not use distributed mode\n",
    "train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset], \n",
    "                                                      samplers=[None, None, None], \n",
    "                                                      batch_size=[config['batch_size_train']] + [\n",
    "                                                          config['batch_size_test']] * 2,\n",
    "                                                      num_workers=[4, 4, 4], is_trains=[True, False, False],\n",
    "                                                      collate_fns=[None, None, None])\n",
    "len(test_dataset), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d8bab-671d-42c7-9c26-c6c221b4f4ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Class DatabaseBuilder (Zilliz-Milvus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ef448-1967-457b-bdd4-050123a7f429",
   "metadata": {},
   "source": [
    "__If haven't built database, run these code__\n",
    "\n",
    "__Else go to class VectorSearcher__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32446036-846e-40fd-a63f-d5dca04a00d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatabaseBuilder:\n",
    "    def __init__(self, collection_name, data, drop=False):\n",
    "        # if drop: drop existing database\n",
    "        check_collection = utility.has_collection(collection_name) if drop else None\n",
    "        drop_result = utility.drop_collection(collection_name) if check_collection else None\n",
    "        self.data = data\n",
    "        self.model_id = model\n",
    "        self.vector_name = 'vector'\n",
    "        self.schema = self.build_schema()\n",
    "        self.index_params = {\n",
    "            \"index_type\": \"AUTOINDEX\",\n",
    "            \"metric_type\": \"IP\",\n",
    "            \"params\": {}\n",
    "        }\n",
    "\n",
    "        self.collection = Collection(\n",
    "            name=collection_name,\n",
    "            schema=self.schema\n",
    "        )\n",
    "\n",
    "    def build_index(self):\n",
    "        self.collection.flush()\n",
    "        self.collection.create_index(\n",
    "            field_name=self.vector_name,\n",
    "            index_params=self.index_params\n",
    "        )\n",
    "        self.collection.load()\n",
    "\n",
    "    def build_schema(self, vector_dim=256):\n",
    "        doc_id = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"doc_id\")\n",
    "        p_ID = FieldSchema(name=\"p_ID\", dtype=DataType.INT16, description=\"p_ID\")\n",
    "        image_path = FieldSchema(name=\"image_path\", dtype=DataType.VARCHAR, description=\"image_path\", max_length=100)\n",
    "        vector = FieldSchema(name=self.vector_name, dtype=DataType.FLOAT_VECTOR, dim=vector_dim)\n",
    "        schema = CollectionSchema(\n",
    "            fields=[doc_id, p_ID, image_path, vector],\n",
    "            auto_id=False,\n",
    "            description=\"Demo collection\"\n",
    "        )\n",
    "        return schema\n",
    "\n",
    "    def make_entities(self, start_index, num_batch = 30000):\n",
    "        tempdata = self.data[start_index: start_index + num_batch]\n",
    "        entities = [tempdata['id'].astype('int64'), tempdata['p_ID'].astype('int16'), tempdata['image_path']]\n",
    "        vectors = tempdata['vector']\n",
    "        entities.append(vectors)\n",
    "        entities = [x.to_numpy() for x in entities]\n",
    "        return entities\n",
    "\n",
    "    def build_database(self):\n",
    "        cur_index = 0\n",
    "        num_batch = 10000\n",
    "        while cur_index < len(self.data):\n",
    "            entities = self.make_entities(cur_index, num_batch)\n",
    "            self.collection.insert(entities)\n",
    "            cur_index += num_batch\n",
    "        self.build_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369111d-902e-44ac-b499-99eb766f1c3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference to get image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8088b3-805c-457c-ad3a-03583bfe8bad",
   "metadata": {},
   "source": [
    "__If have built database, don't run these code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4f40b89-25aa-4d9e-a10a-1e7e1e603399",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3074, 256])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>p_ID</th>\n",
       "      <th>image_path</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0050568427, 0.067113854, -0.06266234, 0.029...</td>\n",
       "      <td>12004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.024172317, 0.06336201, -0.005884793, 0.0051...</td>\n",
       "      <td>12004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.0137637295, -0.015836067, -0.057155482, 0....</td>\n",
       "      <td>12004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.021615053, 0.09207693, 0.05455772, -0.04414...</td>\n",
       "      <td>12005</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.028524911, 0.024824591, 0.051545624, 0.027...</td>\n",
       "      <td>12005</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector   p_ID  \\\n",
       "0  [0.0050568427, 0.067113854, -0.06266234, 0.029...  12004   \n",
       "1  [0.024172317, 0.06336201, -0.005884793, 0.0051...  12004   \n",
       "2  [-0.0137637295, -0.015836067, -0.057155482, 0....  12004   \n",
       "3  [0.021615053, 0.09207693, 0.05455772, -0.04414...  12005   \n",
       "4  [-0.028524911, 0.024824591, 0.051545624, 0.027...  12005   \n",
       "\n",
       "                                          image_path  id  \n",
       "0  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...   0  \n",
       "1  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...   1  \n",
       "2  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...   2  \n",
       "3  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...   3  \n",
       "4  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...   4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_ID = [x['image_id'] for x in test_dataset.ann]\n",
    "image_path = [str(IMAGE_PATH/x['image']) for x in test_dataset.ann]\n",
    "vector_matrix = torch.empty(len(image_path), 256)\n",
    "print(vector_matrix.shape)\n",
    "for image, index in test_loader:\n",
    "    img_emb = model.encode_image(image)['cls_feature'].cpu().detach()\n",
    "    vector_matrix[index] = img_emb\n",
    "dataset_dict = {'vector': list(vector_matrix.numpy()), 'p_ID': person_ID, 'image_path': image_path, 'id': list(range(0, len(person_ID)))}\n",
    "data_table = pd.DataFrame(dataset_dict)\n",
    "data_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "84096155-4e1a-44c5-adff-6aa17dfb0f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vector</th>\n",
       "      <th>p_ID</th>\n",
       "      <th>image_path</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.008985316, 0.030980144, -0.08232519, -0.05...</td>\n",
       "      <td>11004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.042043097, -0.013589532, -0.02761598, -0.0...</td>\n",
       "      <td>11004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>100001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.02434203, 0.054291096, -0.08493884, -0.073...</td>\n",
       "      <td>11004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0067673596, 0.018824464, -0.038445998, -0.1...</td>\n",
       "      <td>11004</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>100003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.02271693, 0.0028380656, -0.12827235, -0.02...</td>\n",
       "      <td>11005</td>\n",
       "      <td>/home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...</td>\n",
       "      <td>100004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vector   p_ID  \\\n",
       "0  [-0.008985316, 0.030980144, -0.08232519, -0.05...  11004   \n",
       "1  [-0.042043097, -0.013589532, -0.02761598, -0.0...  11004   \n",
       "2  [-0.02434203, 0.054291096, -0.08493884, -0.073...  11004   \n",
       "3  [0.0067673596, 0.018824464, -0.038445998, -0.1...  11004   \n",
       "4  [-0.02271693, 0.0028380656, -0.12827235, -0.02...  11005   \n",
       "\n",
       "                                          image_path      id  \n",
       "0  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...  100000  \n",
       "1  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...  100001  \n",
       "2  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...  100002  \n",
       "3  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...  100003  \n",
       "4  /home/jovyan/workspace/BA-PRE_THESIS/DATASET/C...  100004  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_ID = [x['image_id'] for x in val_dataset.ann]\n",
    "image_path = [str(IMAGE_PATH/x['image']) for x in val_dataset.ann]\n",
    "vector_matrix = torch.empty(len(image_path), 256)\n",
    "for image, index in val_loader:\n",
    "    img_emb = model.encode_image(image)['cls_feature'].cpu().detach()\n",
    "    vector_matrix[index] = img_emb\n",
    "dataset_dict_val = {'vector': list(vector_matrix.numpy()), 'p_ID': person_ID, 'image_path': image_path, 'id': list(range(100000,100000 + len(person_ID)))}\n",
    "data_table_val = pd.DataFrame(dataset_dict_val)\n",
    "data_table_val.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109999f-0202-4a56-b177-afccbeac3c6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build database & Insert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c553a7-afe0-44c0-a4d9-05715a47c7ad",
   "metadata": {},
   "source": [
    "__IF HAVEN'T BUILD DATABASE, CHANGE REBUILD_FLAD TO TRUE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ace2ed2-9771-40ce-b187-0cba713b0123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo_collection = 'DEMO_APTM_CUHK_2910'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cab96f21-09c8-44ab-9cfa-f5ae2ff056a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REBUILD_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d109f213-b64d-4238-857b-e78bc5d1eb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if REBUILD_FLAG: #\n",
    "    db_builder = DatabaseBuilder(data=data_table, collection_name=demo_collection, drop=True) #CREATE DATABASE BUILDER\n",
    "    db_builder.build_database() #INSERT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "944eb1d1-6713-4180-a308-083db0f2d374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if REBUILD_FLAG:\n",
    "    db_builder = DatabaseBuilder(data=data_table_val, collection_name=demo_collection, drop=False)\n",
    "    db_builder.build_database() #INSERT VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36aaf4-c953-4c49-ae2e-1502671f5f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Class VectorSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55ecd3df-0999-46f7-b958-e507d255d5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "class VectorSearcher:\n",
    "    def __init__(self, model, client=None, collection_name='DEMO_APTM_CUHK'):\n",
    "        self.model = model\n",
    "        if client is None:\n",
    "            self.client = MilvusClient(\n",
    "                uri=MILVUS_URI,\n",
    "                token=TOKEN\n",
    "            )\n",
    "        else:\n",
    "            self.client = client\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def search(self, texts):\n",
    "        query_infer_dict = self.model.encode_text(texts)\n",
    "        query_emb = query_infer_dict['cls_feature']\n",
    "        #search with batchsize = len(texts)\n",
    "        res = self.client.search(\n",
    "            collection_name = self.collection_name,\n",
    "            data = query_emb.detach().cpu().numpy(),\n",
    "            limit=100,\n",
    "            output_fields = ['p_ID', 'image_path', 'id', 'vector'] \n",
    "        )\n",
    "        return {'text_infer_dict': query_infer_dict, 'result': res}\n",
    "\n",
    "    def filter(self, text, temp_search_result=None, topk=50):\n",
    "        # this function is applied for 1 text\n",
    "        # if has found candidates, don't search again\n",
    "        if temp_search_result is None:\n",
    "            res = self.search(text)\n",
    "        else:\n",
    "            res = temp_search_result\n",
    "        # Get text query embeddings\n",
    "        text_infer_dict = res['text_infer_dict']\n",
    "        text_sequence = text_infer_dict['sequence']\n",
    "        text_attention_mask = text_infer_dict['attention_mask']\n",
    "        # Get candidates image path\n",
    "        result = res['result'][0][:topk]\n",
    "        image_paths = [x['entity']['image_path'] for x in result]\n",
    "        final_result_list = []\n",
    "        probs_result = []\n",
    "        # Infer by cross encoder\n",
    "        for img_path in image_paths:\n",
    "            # Get image sequence again\n",
    "            img = self.model.read_image(img_path).reshape(1, 3, 384, 128)\n",
    "            img_infer_dict = self.model.encode_image(img)\n",
    "            img_sequence = img_infer_dict['sequence']\n",
    "            # Get cross embeddings \n",
    "            cross_emb_cls = self.model.encode_cross_modal(img_sequence, text_sequence, text_attention_mask)['cls_feature']\n",
    "            cross_dict_result = self.model.check_matching(cross_emb_cls)\n",
    "            prob = cross_dict_result['probs'][0][1] # batchsize 1 -> index 0 -> class 1 = Match\n",
    "            # if prob < 0.2:\n",
    "            #     continue\n",
    "            final_result_list.append(img_path)\n",
    "            probs_result.append(prob.cpu().detach().numpy())\n",
    "        sort_index = np.argsort(np.array(probs_result))[::-1]\n",
    "        final_result_list = np.array(final_result_list)[sort_index]\n",
    "        probs_result = np.array(probs_result)[sort_index]\n",
    "        return {\n",
    "            \"image_path\": final_result_list,\n",
    "            \"probs\": probs_result\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6a090205-225a-4631-8cad-df34c51ea8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vsearch = VectorSearcher(model, collection_name=demo_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9af795-783c-4b6e-a3e0-648aa226864c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2041554b-945a-49e3-9241-01a53eb1d4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f6252b00-2145-4504-9a29-9adbf698f6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMP_SEARCH_RESULT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "62328450-8330-4b6a-a808-ccf49b3bf0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, time\n",
    "def generate_random():\n",
    "    curr_time = round(time.time()*1000)\n",
    "    random.seed(curr_time)\n",
    "    k1 = random.randint(0, 2)\n",
    "    # k1 = 0\n",
    "    if k1 == 1:\n",
    "        k = random.randint(0, len(test_dataset))\n",
    "        demo_text = test_dataset.ann[k]['caption'][0]\n",
    "        demo_img = PIL.Image.open(IMAGE_PATH/test_dataset.ann[k]['image']).convert('RGB').resize((124,384))\n",
    "    else:\n",
    "        k = random.randint(0, len(val_dataset))\n",
    "        demo_text = val_dataset.ann[k]['caption'][0]\n",
    "        demo_img = PIL.Image.open(IMAGE_PATH/val_dataset.ann[k]['image']).convert('RGB').resize((124,384))\n",
    "    return demo_text, demo_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a6d9a167-6c8a-4c5b-8eb3-384edb8e1888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gui_candidate_process(texts):\n",
    "    if type(texts) != list: \n",
    "        texts = [texts]\n",
    "    res = vsearch.search(texts)['result'][0]\n",
    "    TEMP_SEARCH_RESULT = res\n",
    "    images = [PIL.Image.open(x['entity']['image_path']) for x in res]\n",
    "    distance = [x['distance'] for x in res]\n",
    "    return list(zip(images, distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c186c6f3-73e3-4df6-bd2c-030ce9a6da3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gui_filter_process(text):\n",
    "    res = vsearch.filter([text], TEMP_SEARCH_RESULT)\n",
    "    img_paths= res['image_path']\n",
    "    probs = [str(x) for x in res['probs']]\n",
    "    images = [PIL.Image.open(img_path) for img_path in img_paths]\n",
    "    return list(zip(images, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d678d35e-7850-4784-97c9-35deece2b2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "Running on public URL: https://1e79423820ce05e74c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1e79423820ce05e74c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        summit_button_0 = gr.Button(value=\"Click to generate random query from test set\")\n",
    "        random_text = gr.Textbox(label=\"Caption of random image\")\n",
    "    # with gr.Row():\n",
    "        random_image = gr.Image(type=\"pil\", height=384, width=184)\n",
    "        \n",
    "    with gr.Row():\n",
    "        query_1 = gr.Textbox(label=\"Text query\")\n",
    "        summit_button_1 = gr.Button(value=\"Ranking candidates by Cosine Similarity\")\n",
    "    with gr.Column(variant=\"panel\"):\n",
    "        with gr.Row():\n",
    "            gallery_1 = gr.Gallery(\n",
    "                label=\"Found images\", show_label=False, elem_id=\"gallery\",\n",
    "                    columns=[3], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "    with gr.Row():\n",
    "        query_2 = gr.Textbox(label=\"Text query\")\n",
    "        summit_button_2 = gr.Button(value=\"Re-ranking by Image-Text Matching probabilities\")\n",
    "    with gr.Column(variant=\"panel\"):\n",
    "        with gr.Row():\n",
    "            gallery_2 = gr.Gallery(\n",
    "                label=\"Found images\", show_label=False, elem_id=\"gallery\",\n",
    "                    columns=[3], rows=[2], object_fit=\"contain\", height=\"auto\")\n",
    "    summit_button_0.click(generate_random, inputs=None, outputs=[random_text, random_image])\n",
    "    summit_button_1.click(gui_candidate_process, inputs=query_1, outputs=gallery_1)\n",
    "    summit_button_2.click(gui_filter_process, inputs=query_2, outputs=gallery_2)\n",
    "\n",
    "demo.launch(inline=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ba136-592f-4d92-b271-2be69a0e04c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
