{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f6cce67-6066-44d2-9f4c-5fe90d19b752",
   "metadata": {},
   "source": [
    "# Set up enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc42d3-d3f1-4500-a9ab-10f12943c0c4",
   "metadata": {},
   "source": [
    "__Define paths & import libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec59511c-9f22-4286-a80e-bda6c928ff77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dscilab_dungvo/workspace/BA-PRE_THESIS/report/training\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(os.getcwd())\n",
    "from pathlib import Path\n",
    "ROOT_PATH = Path('../../paper_clones/TBPS-CLIP').resolve()\n",
    "sys.path.append(str(ROOT_PATH))\n",
    "IMAGE_PATH = Path('../../DATASET').resolve()\n",
    "sys.path.append(str(IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffaed99d-252b-4a43-a42a-bac12e924bac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import wandb\n",
    "import time, datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from misc.build import load_checkpoint, cosine_scheduler, build_optimizer\n",
    "from misc.data import build_pedes_data\n",
    "from misc.eval import test\n",
    "from misc.utils import parse_config, init_distributed_mode, set_seed, is_master, is_using_distributed, AverageMeter\n",
    "from model.tbps_model import clip_vitb, CLIP\n",
    "from options import get_args\n",
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4573daf",
   "metadata": {},
   "source": [
    "# Prepare vietnamese annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16dfa89",
   "metadata": {},
   "source": [
    "__Google translate API 's results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658c87d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dscilab_dungvo/workspace/BA-PRE_THESIS/DATASET/CUHK-PEDES/google_translate.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m google_translates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_PATH\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCUHK-PEDES/google_translate.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m google_translates \u001b[38;5;241m=\u001b[39m google_translates\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m      3\u001b[0m google_translates \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvi: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m google_translates]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dscilab_dungvo/workspace/BA-PRE_THESIS/DATASET/CUHK-PEDES/google_translate.txt'"
     ]
    }
   ],
   "source": [
    "google_translates = open(IMAGE_PATH/'CUHK-PEDES/google_translate.txt', 'r')\n",
    "google_translates = google_translates.readlines()\n",
    "google_translates = [x.replace('\\n', '').replace('vi: ', '') for x in google_translates]\n",
    "len(google_translates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae1a64",
   "metadata": {},
   "source": [
    "__Annotations from a model on Hugging Face__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ea47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anno_path = IMAGE_PATH/'CUHK-PEDES/reid_translate.json'\n",
    "objs = json.load(open(anno_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d219f35",
   "metadata": {},
   "source": [
    "__Split into train, test__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "300123b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, x in enumerate(objs):\n",
    "    x['captions_bt'] = [google_translates[i]]\n",
    "# random.shuffle(objs)\n",
    "\n",
    "ratio = 0.8\n",
    "train_objs = objs[:int(len(objs)*ratio)]\n",
    "test_objs = objs[int(len(objs)*ratio):]\n",
    "\n",
    "for obj in train_objs: obj['split'] = 'train'\n",
    "for obj in test_objs: obj['split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef73f5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "json.dump(train_objs, open(anno_path.parent/'vietnamese'/'train_reid.json', 'w'))\n",
    "json.dump(test_objs, open(anno_path.parent/'vietnamese'/'test_reid.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70c555c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_objs = json.load(open(anno_path.parent/'vietnamese'/'train_reid.json'))\n",
    "test_objs = json.load(open(anno_path.parent/'vietnamese'/'test_reid.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403641c-b658-4ef6-8cf9-5429676caa37",
   "metadata": {},
   "source": [
    "# Set training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbff5ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_path = ROOT_PATH/'config/config.yaml'\n",
    "config = parse_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d3d2476-ea8f-4b9e-add2-5cbb7f4489ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(config)\n",
    "config['log']['print_period'] = 1\n",
    "config['model']['checkpoint'] = ROOT_PATH/'checkpoint/ViT-B-16.pt'\n",
    "config['anno_dir'] = anno_path.parent/'vietnamese'\n",
    "# config['anno_dir'] = ROOT_PATH/'annotation/CUHK-PEDES'\n",
    "\n",
    "config['image_dir'] = IMAGE_PATH/'CUHK-PEDES/imgs'\n",
    "config['device'] = 'cuda'\n",
    "config['model']['use_gather'] = False\n",
    "config['data']['batch_size'] = 120\n",
    "config['model']['saved_path'] = ROOT_PATH/\"checkpoint\"\n",
    "config['experiment']['text_length'] = 132\n",
    "config['model']['embed_dim'] = 512\n",
    "config['schedule']['epoch_warmup'] =2\n",
    "config['schedule']['epoch'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4981942f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meters = {\n",
    "    \"loss\": AverageMeter(),\n",
    "    \"nitc_loss\": AverageMeter(),\n",
    "    \"ss_loss\": AverageMeter(),\n",
    "    \"citc_loss\": AverageMeter(),\n",
    "    \"ritc_loss\": AverageMeter(),\n",
    "    \"mlm_loss\": AverageMeter(),\n",
    "    \"id_loss\": AverageMeter(),\n",
    "}\n",
    "best_rank_1 = 0.0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890998ad-e4c8-45cf-b511-49a15224f1c7",
   "metadata": {},
   "source": [
    "__Build dataloader__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f0d29d-8b13-48e9-bfdc-69959c8b8af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = build_pedes_data(config)\n",
    "train_loader = dataloader['train_loader']\n",
    "test_loader = dataloader['test_loader']\n",
    "num_classes = len(train_loader.dataset.person2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff7a136-301e-40eb-968e-d560892967f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fbff8dcf93491d9866447fdf1f7ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/221 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce764da1d29448880c1a484f513fc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e28daf9614745a2968aac0de326de1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d47e92ec574365ac447e3aa091a9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34544b6a62844f899888cc2db9d5f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a498cc80ba78443eb6dea2a8f33f3368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e0f7a7b1bf44188e364289ae74464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35902e6d785f442a8948e945ff379702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "\n",
    "texts = [\n",
    "    'Three blind horses listening to Mozart.',\n",
    "    'Älgen är skogens konung!',\n",
    "    'Wie leben Eisbären in der Antarktis?',\n",
    "    'Вы знали, что все белые медведи левши?'\n",
    "]\n",
    "model_name = 'M-CLIP/XLM-Roberta-Large-Vit-B-32'\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "embeddings = model.forward(texts, tokenizer)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb2449-6b5a-4502-ad66-7c1e6ef72e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Build model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4159c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "clip_processor = transformers.CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n",
    "clip_b32_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "multilingual_text_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32-multilingual-v1')\n",
    "multilingual_image_model = SentenceTransformer('sentence-transformers/clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf962336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultilingualCLIP(CLIP):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        self.device = 'cuda'\n",
    "        self.initilize_multilingual_encoder()\n",
    "        self.train(True)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    \n",
    "    def initilize_multilingual_encoder(self):\n",
    "        self.vision_encoder  = clip_b32_model.vision_model.to(self.device)\n",
    "        self.vision_proj = clip_b32_model.visual_projection.to(self.device)\n",
    "        self.text_encoder = multilingual_text_model.to(self.device)\n",
    "        for param in self.vision_encoder.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder[0].auto_model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        # for i in range(0, 2):\n",
    "        #     for param in self.text_encoder[0].auto_model.transformer.layer[i].parameters():\n",
    "        #         param.requires_grad = False\n",
    "        class TrickTokenize:\n",
    "            def __call__(self, text, context_length=None):\n",
    "                if type(text) == str:\n",
    "                    text = [text]\n",
    "                self.x = text\n",
    "                return self\n",
    "            def to(self, device=None):\n",
    "                return self.x\n",
    "        def encode_text(texts, return_dense=None):\n",
    "            if return_dense:\n",
    "                return self.text_encoder.encode(texts, convert_to_tensor=True, device=self.device), None\n",
    "            else:\n",
    "                return self.text_encoder.encode(texts, convert_to_tensor=True, device=self.device)\n",
    "        self.encode_text = encode_text\n",
    "        self.tokenize = TrickTokenize() # do nothing because tokenize is done in encode_text\n",
    "        \n",
    "    def encode_image(self, image, return_dense=False):\n",
    "        if return_dense:\n",
    "            return self.vision_proj(self.vision_encoder(image.to(self.device)).pooler_output), None\n",
    "        else:\n",
    "            return self.vision_proj(self.vision_encoder(image.to(self.device)).pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8368679f-ac2a-44a0-a379-fc7f7b39245b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MultilingualCLIP(config, None, None, num_classes, config.experiment.ritc_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a18ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k = random.randint(0, len(train_loader.dataset))\n",
    "# img = train_loader.dataset[k]['image']\n",
    "# import torchvision\n",
    "# normalize = torchvision.transforms.Normalize(\n",
    "#     mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# un_normalize = torchvision.transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225])\n",
    "# def convert_pytorch_img(img):\n",
    "#     import torchvision.transforms.functional as TF\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     return TF.to_pil_image(un_normalize(img))\n",
    "#     # plt.imshow(pil_img)\n",
    "#     # plt.show()\n",
    "# # \n",
    "# image_trasnform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     normalize\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44947c5-ed11-4166-8cee-d5c1aca9b869",
   "metadata": {},
   "source": [
    "__Build opimizer, learning scchduler__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7d485c3-d2c7-4536-8ce7-874cebc7759a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.optim.adamw.AdamW, torch.cuda.amp.grad_scaler.GradScaler)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.schedule.niter_per_ep = len(train_loader)\n",
    "lr_schedule = cosine_scheduler(config)\n",
    "optimizer = build_optimizer(config, model)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "type(lr_schedule), type(optimizer), type(scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba74f59-06b1-406f-aa10-137adf6e755c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up logger wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0313ad58-a358-420d-98ec-837268397916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find TBPS-CLIP_training.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdung-vo20csehcmut\u001b[0m (\u001b[33mtuandung\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'TBPS-CLIP_training.ipynb'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3c95ad-474e-48b5-bcbe-c99d8acd9ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/wandb/run-20231112_101707-5e9wysto</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tuandung/TBPS-CLIP_experiment_9_11/runs/5e9wysto' target=\"_blank\">training_14</a></strong> to <a href='https://wandb.ai/tuandung/TBPS-CLIP_experiment_9_11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tuandung/TBPS-CLIP_experiment_9_11' target=\"_blank\">https://wandb.ai/tuandung/TBPS-CLIP_experiment_9_11</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tuandung/TBPS-CLIP_experiment_9_11/runs/5e9wysto' target=\"_blank\">https://wandb.ai/tuandung/TBPS-CLIP_experiment_9_11/runs/5e9wysto</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "run = wandb.init(\n",
    "    project=\"TBPS-CLIP_experiment_9_11\",\n",
    "    config=config,\n",
    "    name=\"training_\" + '14',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a683fa-13fe-4fec-b36b-a36c465bc3cd",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ec0c64c-b08a-4fe1-be3f-2f967a2638ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import clip\n",
    "# from text_utils.tokenizer import tokenize\n",
    "\n",
    "TEMP = None\n",
    "@torch.no_grad()\n",
    "def test(model, data_loader, max_length, device):\n",
    "    tokenize = model.tokenize\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    dataset = data_loader.dataset\n",
    "    texts = dataset.text\n",
    "    num_text = len(texts)\n",
    "    text_bs = 256\n",
    "\n",
    "    text_feats = []\n",
    "    for i in range(0, num_text, text_bs):\n",
    "        text = texts[i: min(num_text, i + text_bs)]\n",
    "        text = tokenize(text, context_length=max_length).to(device)\n",
    "        text_feat = F.normalize(model.encode_text(text), dim=-1)\n",
    "        \n",
    "        \n",
    "        text_feats.append(text_feat)\n",
    "    text_feats = torch.cat(text_feats, dim=0)\n",
    "\n",
    "    image_feats = []\n",
    "    for image in data_loader:\n",
    "        image = image.to(device)\n",
    "        image_feat = F.normalize(model.encode_image(image), dim=-1)\n",
    "        image_feats.append(image_feat)\n",
    "    image_feats = torch.cat(image_feats, dim=0)\n",
    "\n",
    "    sims_matrix = text_feats @ image_feats.t()\n",
    "    eval_result = metric_eval(sims_matrix, dataset.img2person, dataset.txt2person)\n",
    "\n",
    "    return eval_result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def metric_eval(scores_t2i, img2person, txt2person):\n",
    "    device = scores_t2i.device\n",
    "    img2person = img2person.to(device)\n",
    "    txt2person = txt2person.to(device)\n",
    "\n",
    "    index = torch.argsort(scores_t2i, dim=-1, descending=True)\n",
    "    pred_person = img2person[index]\n",
    "    matches = (txt2person.view(-1, 1).eq(pred_person)).long()\n",
    "\n",
    "    def acc_k(matches, k=1):\n",
    "        matches_k = matches[:, :k].sum(dim=-1)\n",
    "        matches_k = torch.sum((matches_k > 0))\n",
    "        return 100.0 * matches_k / matches.size(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    ir1 = acc_k(matches, k=1).item()\n",
    "    ir5 = acc_k(matches, k=5).item()\n",
    "    ir10 = acc_k(matches, k=10).item()\n",
    "    ir_mean = (ir1 + ir5 + ir10) / 3\n",
    "\n",
    "    real_num = matches.sum(dim=-1)\n",
    "    tmp_cmc = matches.cumsum(dim=-1).float()\n",
    "    order = torch.arange(start=1, end=matches.size(1) + 1, dtype=torch.long).to(device)\n",
    "    tmp_cmc /= order\n",
    "    tmp_cmc *= matches\n",
    "    AP = tmp_cmc.sum(dim=-1) / real_num\n",
    "    mAP = AP.mean() * 100.0\n",
    "\n",
    "    eval_result = {'r1': ir1,\n",
    "                   'r5': ir5,\n",
    "                   'r10': ir10,\n",
    "                   'r_mean': ir_mean,\n",
    "                   'mAP': mAP.item()\n",
    "                   }\n",
    "\n",
    "    return eval_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a10293-1506-4824-b034-87a620fc1d20",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b59684e1-fd59-4f7c-94a4-f08a9ec2977c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration[1/268], loss: 6.4066, nitc_loss: 4.5779, ss_loss: 2.0207, citc_loss: 0.0004, ritc_loss: -0.1924, Base Lr: 1.00e-06\n",
      "Epoch[1] Iteration[2/268], loss: 6.4046, nitc_loss: 4.5606, ss_loss: 2.0388, citc_loss: 0.0005, ritc_loss: -0.1953, Base Lr: 1.19e-06\n",
      "Epoch[1] Iteration[3/268], loss: 6.3839, nitc_loss: 4.5439, ss_loss: 2.0328, citc_loss: 0.0006, ritc_loss: -0.1934, Base Lr: 1.37e-06\n",
      "Epoch[1] Iteration[4/268], loss: 6.4099, nitc_loss: 4.5462, ss_loss: 2.0597, citc_loss: 0.0005, ritc_loss: -0.1965, Base Lr: 1.56e-06\n",
      "Epoch[1] Iteration[5/268], loss: 6.4025, nitc_loss: 4.5867, ss_loss: 2.0041, citc_loss: 0.0006, ritc_loss: -0.1889, Base Lr: 1.74e-06\n",
      "Epoch[1] Iteration[6/268], loss: 6.3180, nitc_loss: 4.5351, ss_loss: 1.9704, citc_loss: 0.0005, ritc_loss: -0.1880, Base Lr: 1.93e-06\n",
      "Epoch[1] Iteration[7/268], loss: 6.2750, nitc_loss: 4.5237, ss_loss: 1.9398, citc_loss: 0.0007, ritc_loss: -0.1892, Base Lr: 2.11e-06\n",
      "Epoch[1] Iteration[8/268], loss: 6.2599, nitc_loss: 4.4974, ss_loss: 1.9507, citc_loss: 0.0008, ritc_loss: -0.1889, Base Lr: 2.30e-06\n",
      "Epoch[1] Iteration[9/268], loss: 6.1971, nitc_loss: 4.4845, ss_loss: 1.8966, citc_loss: 0.0008, ritc_loss: -0.1848, Base Lr: 2.48e-06\n",
      "Epoch[1] Iteration[10/268], loss: 6.1517, nitc_loss: 4.4697, ss_loss: 1.8635, citc_loss: 0.0010, ritc_loss: -0.1825, Base Lr: 2.67e-06\n",
      "Epoch[1] Iteration[11/268], loss: 6.1832, nitc_loss: 4.4833, ss_loss: 1.8776, citc_loss: 0.0010, ritc_loss: -0.1786, Base Lr: 2.85e-06\n",
      "Epoch[1] Iteration[12/268], loss: 6.1235, nitc_loss: 4.5029, ss_loss: 1.8032, citc_loss: 0.0012, ritc_loss: -0.1839, Base Lr: 3.04e-06\n",
      "Epoch[1] Iteration[13/268], loss: 6.0351, nitc_loss: 4.4394, ss_loss: 1.7762, citc_loss: 0.0012, ritc_loss: -0.1817, Base Lr: 3.22e-06\n",
      "Epoch[1] Iteration[14/268], loss: 5.9302, nitc_loss: 4.3194, ss_loss: 1.7788, citc_loss: 0.0012, ritc_loss: -0.1692, Base Lr: 3.41e-06\n",
      "Epoch[1] Iteration[15/268], loss: 6.0386, nitc_loss: 4.3735, ss_loss: 1.8293, citc_loss: 0.0014, ritc_loss: -0.1656, Base Lr: 3.59e-06\n",
      "Epoch[1] Iteration[16/268], loss: 5.8652, nitc_loss: 4.2983, ss_loss: 1.7327, citc_loss: 0.0017, ritc_loss: -0.1675, Base Lr: 3.78e-06\n",
      "Epoch[1] Iteration[17/268], loss: 5.8536, nitc_loss: 4.3591, ss_loss: 1.6604, citc_loss: 0.0020, ritc_loss: -0.1678, Base Lr: 3.96e-06\n",
      "Epoch[1] Iteration[18/268], loss: 5.8554, nitc_loss: 4.3374, ss_loss: 1.6718, citc_loss: 0.0019, ritc_loss: -0.1558, Base Lr: 4.15e-06\n",
      "Epoch[1] Iteration[19/268], loss: 5.8763, nitc_loss: 4.3311, ss_loss: 1.7026, citc_loss: 0.0023, ritc_loss: -0.1597, Base Lr: 4.33e-06\n",
      "Epoch[1] Iteration[20/268], loss: 5.7169, nitc_loss: 4.2697, ss_loss: 1.5904, citc_loss: 0.0025, ritc_loss: -0.1457, Base Lr: 4.52e-06\n",
      "Epoch[1] Iteration[21/268], loss: 5.7130, nitc_loss: 4.2541, ss_loss: 1.6006, citc_loss: 0.0023, ritc_loss: -0.1440, Base Lr: 4.70e-06\n",
      "Epoch[1] Iteration[22/268], loss: 5.7980, nitc_loss: 4.2589, ss_loss: 1.6816, citc_loss: 0.0024, ritc_loss: -0.1449, Base Lr: 4.89e-06\n",
      "Epoch[1] Iteration[23/268], loss: 5.6931, nitc_loss: 4.2144, ss_loss: 1.5966, citc_loss: 0.0025, ritc_loss: -0.1204, Base Lr: 5.07e-06\n",
      "Epoch[1] Iteration[24/268], loss: 5.5971, nitc_loss: 4.1477, ss_loss: 1.5697, citc_loss: 0.0027, ritc_loss: -0.1230, Base Lr: 5.26e-06\n",
      "Epoch[1] Iteration[25/268], loss: 5.7436, nitc_loss: 4.2856, ss_loss: 1.5824, citc_loss: 0.0031, ritc_loss: -0.1274, Base Lr: 5.44e-06\n",
      "Epoch[1] Iteration[26/268], loss: 5.5891, nitc_loss: 4.1842, ss_loss: 1.5232, citc_loss: 0.0032, ritc_loss: -0.1215, Base Lr: 5.63e-06\n",
      "Epoch[1] Iteration[27/268], loss: 5.4948, nitc_loss: 4.0575, ss_loss: 1.5435, citc_loss: 0.0032, ritc_loss: -0.1094, Base Lr: 5.81e-06\n",
      "Epoch[1] Iteration[28/268], loss: 5.5427, nitc_loss: 4.1235, ss_loss: 1.5236, citc_loss: 0.0032, ritc_loss: -0.1075, Base Lr: 6.00e-06\n",
      "Epoch[1] Iteration[29/268], loss: 5.5185, nitc_loss: 4.1456, ss_loss: 1.4735, citc_loss: 0.0036, ritc_loss: -0.1042, Base Lr: 6.18e-06\n",
      "Epoch[1] Iteration[30/268], loss: 5.5035, nitc_loss: 4.1308, ss_loss: 1.4696, citc_loss: 0.0038, ritc_loss: -0.1006, Base Lr: 6.37e-06\n",
      "Epoch[1] Iteration[31/268], loss: 5.4047, nitc_loss: 4.0922, ss_loss: 1.4110, citc_loss: 0.0039, ritc_loss: -0.1024, Base Lr: 6.55e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb Cell 31\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# scaler.scale(loss).backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# scaler.step(optimizer)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# scaler.update()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# model.zero_grad()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# if (i % 5 == 0) or (i == len(train_loader) - 1):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://tunnel%2Bvscode-server/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/7.TBPS-CLIP_training_vietnammese.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m it \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "logger = run\n",
    "for epoch in range(config.schedule.epoch):\n",
    "    start_time = time.time()\n",
    "    for meter in meters.values():\n",
    "        meter.reset()\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_schedule[it] * param_group['ratio']\n",
    "        if epoch == 0:\n",
    "            alpha = config.model.softlabel_ratio * \\\n",
    "                min(1.0, i / len(train_loader))\n",
    "        else:\n",
    "            alpha = config.model.softlabel_ratio\n",
    "\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "            ret = model(batch, alpha)\n",
    "            loss = sum([v for k, v in ret.items() if \"loss\" in k])\n",
    "        batch_size = batch['image'].shape[0]\n",
    "        meters['loss'].update(loss.item(), batch_size)\n",
    "        meters['nitc_loss'].update(ret.get('nitc_loss', 0), batch_size)\n",
    "        meters['ss_loss'].update(ret.get('ss_loss', 0), batch_size)\n",
    "        meters['citc_loss'].update(ret.get('citc_loss', 0), batch_size)\n",
    "        meters['ritc_loss'].update(ret.get('ritc_loss', 0), batch_size)\n",
    "        meters['mlm_loss'].update(ret.get('mlm_loss', 0), batch_size)\n",
    "        meters['id_loss'].update(ret.get('id_loss', 0), batch_size)\n",
    "        batch_size = batch['image'].shape[0]\n",
    "        logger.log({\n",
    "            'epoch': epoch, \n",
    "            'step': i,\n",
    "            'lr': lr_schedule[it],\n",
    "            **{k: v.avg for k, v in meters.items()}\n",
    "        })       \n",
    "\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        # model.zero_grad()\n",
    "\n",
    "        # if (i % 5 == 0) or (i == len(train_loader) - 1):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        it += 1\n",
    "        if (i + 1) % config.log.print_period == 0:\n",
    "            info_str = f\"Epoch[{epoch + 1}] Iteration[{i + 1}/{len(train_loader)}]\"\n",
    "            # log loss\n",
    "            for k, v in meters.items():\n",
    "                if v.val != 0:\n",
    "                    info_str += f\", {k}: {v.val:.4f}\"\n",
    "            info_str += f\", Base Lr: {param_group['lr']:.2e}\"\n",
    "            print(info_str)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_per_batch = (end_time - start_time) / (i + 1)\n",
    "    time_epoch = end_time - start_time\n",
    "    print(\"Epoch {} done. Time per batch: {:.3f}[s] Speed: {:.1f}[samples/s]\"\n",
    "          .format(epoch + 1, time_per_batch, train_loader.batch_size / time_per_batch))\n",
    "\n",
    "    eval_result = test(\n",
    "        model, dataloader['test_loader'], config['experiment']['text_length'], config.device)\n",
    "    rank_1, rank_5, rank_10, map = eval_result['r1'], eval_result['r5'], eval_result['r10'], eval_result['mAP']\n",
    "    logger.log({\n",
    "        'epoch': epoch, \n",
    "        'rank_1': rank_1,\n",
    "        'rank_5': rank_5,\n",
    "        'rank_10': rank_10,\n",
    "        'mAP': map,\n",
    "        'epoch_time': time_epoch,\n",
    "    })\n",
    "\n",
    "    print('Acc@1 {top1:.5f} Acc@5 {top5:.5f} Acc@10 {top10:.5f} mAP {mAP:.5f}'.format(top1=rank_1, top5=rank_5,\n",
    "                                                                                      top10=rank_10, mAP=map))\n",
    "    torch.cuda.empty_cache()\n",
    "    if best_rank_1 < rank_1:\n",
    "        best_rank_1 = rank_1\n",
    "        best_epoch = epoch\n",
    "\n",
    "        save_obj = {\n",
    "            'model': model.module.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'config': config,\n",
    "        }\n",
    "        torch.save(save_obj, os.path.join(\n",
    "            config.model.saved_path, 'checkpoint_best_18_11.pth'))\n",
    "\n",
    "print(f\"best Acc@1: {best_rank_1} at epoch {best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d82e1780-67ef-46b3-8522-0aa7754a2815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m res \u001b[38;5;241m=\u001b[39m test(model, dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loader\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_length\u001b[39m\u001b[38;5;124m'\u001b[39m], config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 2\u001b[0m rank_1, rank_5, rank_10, \u001b[38;5;28mmap\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43meval_result\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr1\u001b[39m\u001b[38;5;124m'\u001b[39m], eval_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr5\u001b[39m\u001b[38;5;124m'\u001b[39m], eval_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr10\u001b[39m\u001b[38;5;124m'\u001b[39m], eval_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmAP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_result' is not defined"
     ]
    }
   ],
   "source": [
    "res = test(model, dataloader['test_loader'], config['experiment']['text_length'], config.device)\n",
    "rank_1, rank_5, rank_10, map = eval_result['r1'], eval_result['r5'], eval_result['r10'], eval_result['mAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "327e5c46-56f1-4285-a01a-850c08658865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r1': 0.5098234415054321,\n",
       " 'r5': 1.8776423931121826,\n",
       " 'r10': 3.2081573009490967,\n",
       " 'r_mean': 1.8652077118555705,\n",
       " 'mAP': 1.310916543006897}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b424a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbps-clip",
   "language": "python",
   "name": "tbps-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
