Traceback (most recent call last):
  File "/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/TBPR-CLIP_vietnamese_training.py", line 226, in <module>
    ret = model(batch, alpha)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jovyan/workspace/BA-PRE_THESIS/paper_clones/TBPS-CLIP/model/tbps_model.py", line 123, in forward
    aug2_embed = self.simclr_mlp(self.encode_image(input['aug_ss_2'].to(self.config.device)))
  File "/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/TBPR-CLIP_vietnamese_training.py", line 113, in encode_image
    return self.vision_proj(self.vision_encoder(image.to(self.device)).pooler_output)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 846, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 632, in forward
    layer_outputs = encoder_layer(
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 372, in forward
    hidden_states, attn_weights = self.self_attn(
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 321, in forward
    attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 9.77 GiB already allocated; 22.75 MiB free; 10.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF