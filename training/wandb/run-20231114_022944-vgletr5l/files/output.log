Traceback (most recent call last):
  File "/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/TBPR-CLIP_vietnamese_training.py", line 226, in <module>
    ret = model(batch, alpha)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jovyan/workspace/BA-PRE_THESIS/paper_clones/TBPS-CLIP/model/tbps_model.py", line 156, in forward
    image_1_features = self.encode_image(images_1)
  File "/home/jovyan/workspace/BA-PRE_THESIS/REPORT/training/TBPR-CLIP_vietnamese_training.py", line 114, in encode_image
    return self.vision_proj(self.vision_encoder(image.to(self.device)).pooler_output)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 846, in forward
    encoder_outputs = self.encoder(
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 632, in forward
    layer_outputs = encoder_layer(
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 372, in forward
    hidden_states, attn_weights = self.self_attn(
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/transformers/models/clip/modeling_clip.py", line 261, in forward
    query_states = self.q_proj(hidden_states) * self.scale
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/envs/tbps-clip/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 9.51 GiB already allocated; 10.75 MiB free; 10.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF